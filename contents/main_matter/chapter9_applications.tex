\chapter{Application of Acoustic Echoes}\label{ch:application}

\newthought{Synopsis} \synopsisChApplication

The material presented here results from the personal elaboration of concepts and references available in the literature.
However, since this literature is vast and spans diverse decades of scientific research, we do not aim to cover it entirely. We instead illustrate the relation between indoor sound propagation and how this information is accounted for in classical methods.
Furthermore, some definitions are digested from classical textbooks already used for this thesis, such as~\citeonly{vincent2018audio}, and audio signal processing lecture notes.

\section{Audio Scene Analysis Problems}
As mentioned in the first chapter, the audio scene analysis aims to parcel all the relevant information in the indoor audio scene.
Different types of information are estimated or inferred by different audio signal processing algorithms solving specific problems.
Despite their diversity, most of these problems can be defined on a common model for the microphone observations.

\mynewline
Let there be a meeting room with well-defined geometry.
In it, $\numSrcs$ sound sources are located at determined positions, such as some speakers chatting while standing in the room.
As it is a indoor scenario, all the elements of reverberation (in particular echoes) are presents.
Diffuse background noise is present as well, for instance, due to the air conditioner or car traffic outside.
This whole audio scene is recorded by a device featuring a microphone array of $\numMics$ sensors.
Furthermore we assume a static far field scenario and we model each $\idxSrc$ sources and $\idxMic$ microphone as well-defined points with coordinate $\positionSource$ and $\positionMicrophone$, respectively.
This is a reasonable assumption in the context of table-top devices, such as smart home devices.

\begin{figure}
    aoeu
\end{figure}

Recalling the (discrete) time-domain signal model~\cref{ch:processing:blabal} already discussed the relative chapter, the signal recorded at the $\idxMic$-th microphones reads
\begin{equation}
    \label{eq:application:stft}
    \mic_\idxMic[n] = \sum_{\idxSrc = 1}^{\numSrcs}
        \kparen{\flt_{\idxMicSrc}( \positionMicrophone_{\idxMic}  | \positionSource_{\idxSrc}) \convDis \src_{\idxSrc}} [n] + \nse_\idxMic[n]
    .
\end{equation}
Note that the filter $\flt_{\idxMicSrc}(\positionMicrophone_{\idxMic} | \positionSource_{\idxSrc})$ denotes the \RIR/ where we intentionally highlight the dependencies on geometry,
namely, accounting for the whole sound propagation for the source position $\positionSource_{\idxSrc}$ to the microphone position $\positionMicrophone_{\idxMic}$.
In fact, as discussed throughout~\cref{ch:acoustics,ch:processing}, we can decouple the information of indoor microphone natural recordings into two orthogonal contributions:
the \RIRs/ (thus the mixing matrix) accounting for only the sound propagation, and the source signals that depend only on the sound content.

\newcommand{\setMicSignals}{\ensuremath{\set{\mic_{\idxMic}}_\idxMic}}
\newcommand{\setSrcSignals}{\ensuremath{\set{\src_{\idxSrc}}_\idxSrc}}
\newcommand{\setSrcPositions}{\ensuremath{\set{\positionSource_{\idxSrc}}_\idxSrc}}
\newcommand{\setFltSignals}{\ensuremath{\set{\flt_{\idxMicSrc}(\positionMicrophone_{\idxMic} | \positionSource_{\idxSrc})}_{\idxMicSrc}}}


\newthought{The Audio Scene Analysis Problems} presented already in the introductory chapter (See~\cref{sec:intro:scene}) can be now extended and rewritten in terms of the above notation.
Furthermore, we will consider here the only ones directly addressed in this thesis, namely, room impulse response estimation, audio source separation, spatial filtering, sound source localization, and room geometry estimation.

\begin{table}[!h]

    \begin{fullwidth}
    \centering
    \small
    \renewcommand{\arraystretch}{1.3}

    \input{tables/application/tab_inverseproblem.tex}
    \caption{List of audio scene analysis problems considered in this thesis accompanied with their mathematical description.}
    \label{tab:processing:problems}

    \end{fullwidth}

\end{table}

%% blind-informed, active-passive
As introduced in~\cref{sec:estimation:problem}, this problems exist in their active and passive as well as informed and blind scenario.



\section{Literature review: an acoustic perspective}
Bibliography with respect to sound propagation
\begin{itemize}
    \item Ignored
    \item Anechoic Phat
    \item Fully modeled
    \item Early echoes
\end{itemize}

\subsection{Literature review: an algorithmic perspective}\cite{subsec:application:algos}
Bibliography with respect to learning and knowledge approaches


Based on this idea, so-called \textit{echo-aware} methods have been introduced few decades ago, where matched filters (or rake receivers) are used to constructively sum the sound reflections \citeonly{Jan1995matched, Affes1997signal} and build beamformers achieving much better sound qualities \citeonly{gannot2001signal}.
This methods have recently regained interested as manifested by the European project SCENIC~\citeonly{Annibale2011scenic} and the UK research \href{http://www.s3a-spatialaudio.org/}{S$^3$A project}.
They show that knowing the properties of a few early echoes can boosts performances of typical indoor audio inverse problems such as speech enhancement (SE) \citeonly{Dockmanic2015raking, Kowalczyk2019raking}, sound source localization \citeonly{ribeiro2010turning, DiCarlo2019mirage}, and separation \citeonly{scheibler2017separake, leglaive2016multichannel}.
Another fervent area of research spanning transversely the audio and acoustic signal processing fields is estimating the room geometry blindly from acoustic signals.
As presented by Crocco \textit{et al.} in \citeonly{crocco2017uncalibrated}, the end-to-end room geometry estimation (RooGE) involves many subsequent subtasks:
RIR estimation, peak picking, microphones calibration, echo labeling, reflectors estimation. Acoustic echo retrieval (AER) is common to many of these topics. It consists in estimating the properties of echoes such as their TOAs and energies. The former problem is referred to as TOA estimation, or time-delay estimation when the direct-path is taken as reference. Furthermore, as interesting applications, these methods have been recently used in active scenarios, namely knowing the transmitted signals, using unmanned aerial vehicle (UAV, a.k.a. drones) \citeonly{jensen2019method, Boutin2020drone} and mobile-phones \citeonly{Shih2019phone}.


\cref{sec:application:scenario}
\cref{sec:application:sota}
\cref{sec:application:echosota}

%% problems %%
\newthoughtpar{Separation \vs/ Enhancement}

%% models %%
\newthoughtpar{end2end \vs/ 2step}
end2end: from data to (feature to) target
\\2-step: (from data to features) + features to target

\newthoughtpar{Knowledge-based \vs/ Learning-based}
\begin{itemize}
    \item Bottom-up vs Top-down information processing
    \item Knowledge-based: specialized signal processing and mathematical algorithms informed by knowledge;
    \item Learning-based: machine learning usually trained in supervised fashion.
\end{itemize}

\newthoughtpar{Supervised \vs/ Unsupervised}
\newthoughtpar{Machine Learning \vs/ Deep Learning}


\section{Problem Formulation}
Given the narrowband \STFT/ signal model presented is~\cref{subsec:processing:model:stft}, the signals captured by $\numMics$ microphone listening to a single sound source ($\numSrcs=1$) in a noisy reverberant room, reads:
\begin{equation}
    \MICS[k,l] = \FLTS[k] \SRC[k, l]+ \NSES[k,l]
    ,
\end{equation}


\newthought{Sound Source Separation}

\newthought{Spatial Filtering}

\newthought{Localization}




\section{Separation, Spatial Filtering and Localization}
The knowledge of early echoes should boost audio scene analysis task.

\subsection{Speech Source Separation}
Audio source separation refers to the process of extracting acoustic signals from mixtures featuring target and interfering sounds.
The ability of focusing on a selected source only, while filtering out the rest, is known as \textit{the cocktail party effect}~\citeonly{cherry1953cocktail, bee2008cocktail}
The scenario where no prior knowledge is available about both the sources and the mixing process, is usually referred to as \textit{blind sound separation}.
When it is applied to approximate human hearing, such as using only stereophonic mixture and considering the human auditory system, it is known as \CASAdef/.

[Remaggi thesis]
Two of the mostly investigated areas study either musical or speech recordings [Vincent et al., 2012].
Although music source separation represents a key part of the audio community [Ewert et al., 2014], the focus of this thesis is on speech source separation.
This is of interest for several audio applications, such as: speech enhancement [Mohammadiha et al., 2013], crosstalk cancellation [Akeroyd et al., 2007], hearing aids [Healy et al., 2013], and automatic speech recognition [Li et al., 2014]

Echoes have been used previously to enhance various audio processing tasks.

It was shown that they improve indoor beamforming \citeonly{Dokmanic:2015dr, Scheibler:2015ii, RobinThesis},
aid in sound source localization \citeonly{Ribeiro:2010uj},
and enable low-resource microphone array self-localization \citeonly{Dokmanic:2016gu}.

[Remaggi thesis]
During the last twenty years, speech separation has gained quite of attention. Some of the proposed methods achieved source separation exploiting the availability of a single microphone [Jang and Lee, 2003, Radfar and Dansereau, 2007, Schmidt and Olsson, 2006]. However, they were limited by the amount of information utilised. Therefore, other methods attempted the separation process by employing multichannel microphone arrays. These methods are classically categorised into three main groups, depending on the type of approach undertaken [Vincent et al., 2012]: the beamformers [Araki et al., 2003, Coleman et al., 2015a, VanVeen and Buckley, 1988]; the independent component analysis (ICA) based [Bell and Sejnowski, 1995, Cardoso, 1998, Makino et al., 2007]; the time-frequency (TF) mask based [Alinaghi et al., 2014, Deleforge et al., 2015, Mandel et al., 2010, Sawada et al., 2011, Yilmaz and Rickard, 2004]. A visualisation of these three

\subsection{Speech Source Localization}
\subsection{Spatial Filtering}
Beamformers combines the channels of multiple microphones in order to achieve spatial selectivity, suppressing noise, interferences and reverberation~\citeonly{VanTrees2004Optimum}.

\subsection{Room Geometry Estimation}