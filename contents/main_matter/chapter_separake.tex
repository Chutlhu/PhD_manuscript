\chapter{\library{Separake}: Echo-aware Sound Source Separation}\label{ch:separake}
\openepigraph{Please could you stop the noise, \\
    I'm trying to get some rest\\
From all the unborn chicken voices in my head.}{Radiohead, \textit{Paranoid Android}}

\vspace{-2.5em}
\newthought{Synopsis} \marginpar{%
    \footnotesize
    \textbf{Keywords:} Blind Channel Identification, Super Resolution, Sparsity, Acoustic Impulse Response.
    \\\textbf{Resources:}
    \begin{itemize}
        \item \href{https://doi.org/10.1109/ICASSP.2018.8461345}{Paper}
        \item \href{https://github.com/fakufaku/separake}{Code}
        \item \href{https://sigport.org/documents/separake-source-separation-little-help-echoes}{Slides}
\end{itemize}
} \synopsisChSeparake

\mynewline
The material presented in this chapter results from a collaboration with Robin Scheibler and Ivan Dokmani\'{c} and was published in~\cite{scheibler2018separake}.
This chapter recalls the main findings of the paper and brings additional insights on the literature and on the proposed model, which has been re-written using this thesis' notations.
The personal contribution to this collaboration, done in the early months of the \PhD/, was adapting and implementing in Python of the proposed \ac{NMF} method accounting for echoes and using pre-trained dictionaries.

\section{Literature review in echo-aware Multichannel Audio Source Separation}\label{sec:separake:sota}
The scientific literature on \ac{MASS} is vast, still active, and spans decades.
For a brief introduction, the reader can refer to~\cref{sec:application:separation} and to recent books~\citeonly{vincent2018audio,makino2018audio}.
In this chapter we will consider only the case of multichannel convolutive recordings featuring reverberant speech data in overdetermined settings.
Even selecting this narrow case, the literature remain vast and we will not review it in the context of this thesis.

\mynewline
Here, we will focus on \acf{NMF}-based \ac{MASS}, which allows to easily incorporate side information on both the sources and the acoustic propagation.
This is a key feature, as opposed to the end-to-end \acf{DNN}-model.
These latter models are very popular nowadays and are able to reach impressive performances in the general case.
However, when it comes include side information or adapt to specific tasks, it is far from trivial.

\mynewline
To the best author knowledge, in the literature only few works can be found that incorporate the knowledge of echoes into sound source separation.
% In the work \citeonly{huang2005blind}, the authors proposes a decomposition of the source separation problem into different steps.
% First they estimate the \acp{RIR} by extending the \SIMO/-\BCE/ framework form \MIMO/ systems.
% Here the \acp{RIR} are modeled as \FIR/ filters following the multipath echo model.
% Secondly, the estimated filters are used to build the demixing matrix, and then to separate the sources with an inverse-filtering approach.
% However, this method exhibits an high computational cost, which was addressed later in~\citeonly{rotili2010joint}.
% Nevertheless these approaches were shown to lack robustness in low SNR conditions.
The work in~\citeonly{asaei2014structured} proposes geometry-based approach embedded in a sparse optimization framework.
First, by localizing the image sources and estimating the room geometry, the echoes' timings are estimated.
Then, after computing the echoes' coefficients in a convex optimization framework, the individual speech signals are separated with either inverse-filtering or sparse recovery.
The performance of this approach relies on the \ac{RIR} and geometry estimation steps, which are very sensitive to the challenging acoustic condition, \eg/ low SNR or high \RT.
\\Instead, the work in \citeonly{leglaive2015multichannel} proposes to tackle the convolutive model by imposing a probabilistic prior on the early part of the \acp{RIR}, namely, modeled as an autoregressive process in the frequency domain.
Later, the same authors extended this work in~\citeonly{leglaive2016multichannel} accounting for both early and late part of the mixing filters.


\begin{figure}[]
    \begin{sidecaption}[Separake setup]{%
        Typical setup with two speakers recorded by two microphones.
        The illustration shows the virtual microphone model (grey microphones) with direct sound path (dotted lines) and resulting first-order echoes (colored lines).
        \\Image taken from our paper~\citeonly{scheibler2018separake}.
        }[fig:separake:setup]
    \centering
    \includegraphics[width=.6\linewidth]{separake/separake.pdf}
    \end{sidecaption}
\end{figure}

\newthought{The proposed approach} is yet different from those presented above.
First, rather than fitting the echo model as in~\citeonly{leglaive2015multichannel,leglaive2016multichannel}, or estimating the mixing filters as in~\citeonly{asaei2014structured},
we aim to show that separation in the presence of known echoes is better than separation without echoes.
Second, we conduct this investigation in the context of source separation with non-negative source models with a deterministic model for the mixing filters.
Third, we propose to solve the problem from the point of view of \textit{image microphones}, already used in~\citeonly{bergamo2004collaborative,korhonen2008acoustic}.
The image microphone model is equivalent to the \ISMdef/~\citeonly{allen1979image}, where virtual receivers are placed outside of the room (See~\cref{fig:separake:setup}).
Even if the \ac{ISM} is more common and implemented in practice in acoustic simulators, the two models are strictly equivalent.
% Therefore, this is a reasonable model for early echoes when strongly reflective, non-diffuses surfaces are present near the sources and microphones, which commonly happens in a living room or conference room.
% However, it incurs a significant mismatch with respect to the complete reverberation (See~\cref{ch:acoustics}).
This approach is based on the acoustic \textit{rake receivers}\sidenote{
    Rake receivers are a particular type of beamformers accounting for the effect of multipath propagation (see~\cref{sec:dechorateapp:se})
} previously proposed in~\citeonly{dokmanic2015raking} and is thus dubbed \library{Separake}.
This model allows for a simple and intuitive way to structure the mixing matrix so that less parameters need to be estimated.

\mynewline
The considered setup is illustrated in~\cref{fig:separake:setup}.
We assume that the array is placed close to a wall or a corner.
This is useful for the following reasons:
first, it makes echoes from the nearby walls significantly stronger than all other echoes;
second, it ensures that the resulting image array (real and image microphones) is compact, allowing to assume the far field regime.
This assumption is motivated by typical echo-aware signal processing applications, such as for smart-home devices, typically placed on a table or close to a wall.

\newthought{Translating echoes into image arrays} provides an interesting geometrical interpretation in light of beamforming theory.
Real and virtual microphones form dipoles with diverse frequency-dependent directivity patterns.
By integrating more and more virtual microphones, the directivity patterns change and higher spatial selectivity can be achieved~\citeonly{dokmanic2015raking}.
This effect is shown in~\cref{fig:separake:directivity}.
Therefore, the goal of this work is to design audio source separation algorithms which benefit from this known spatial diversity.
Note that the signal virtual microphones are not accessible directly, as the observations will remain the mixture of convolutive sound at the real microphones.
However, such formulation rather offer a useful interpretation and a way to model incoming reverberant signals.

\begin{figure}
    \begin{fullwidth}
    \centering
    \resizebox{\linewidth}{!}{
        \input{figures/separake/dipole_tripoles.tikz}
    }
    \caption{The frequency-dependent directivity pattern for a virtual array built with a real and one (left) and two (right) image microphones.
    Image taken from the \href{https://sigport.org/documents/separake-source-separation-little-help-echoes}{presentation} of the paper~\citeonly{scheibler2018separake}.}
    \label{fig:separake:directivity}
    \end{fullwidth}
\end{figure}

\section{Modeling}
Recalling the echo model for the \acp{RIR}, and assuming $\numEchs$ echoes per source are known, the approximate \RTFdef/ from source $\idxSrc$ to microphone $\idxMic$ writes
\begin{equation}
    \label{eq:separake:approx_tf}
    \tilde{H}_{\idxMicSrc}(f) = \sum_{\idxEch=0}^{\numEchs} \alpha_{\idxMicSrc}^{(\idxEch)} \cste^{-\csti 2 \pi f \tau_{\idxMicSrc}^{(\idxEch)}}.
\end{equation}
Absolute \acsp{TOA} relate to the source's distance which is not assumed to be known here.
Instead, we will assume that only the \acp{TDOA} are known, by arbitrarily fixing the delay of the direct path of a reference microphone to zero.
In addition, we assume all walls to be spectrally flat in the frequency range of interest and that $\alpha_{\idxMicSrc}^{(\idxEch)}$ are known up to a scaling (i.e. $\alpha_{\idxMicSrc}^{(0)} = 1$).
In this work all these echoes properties are assumed to be known.

\mynewline
Assuming the narrowband approximation, the mixing process can be modeled as in \cref{subsec:processing:model:stft}.
That is, the \STFTdef/ of the $\idxMic$-th microphone signal reads
\begin{equation}
    \label{eq:separake:stft}
    \MIC_\idxMic[k,l] = \sum_{\idxSrc = 1}^{\numSrcs} H_{\idxMicSrc}[k] \SRC_{\idxSrc}[k,l] + \NSE_\idxMic[k,l]
\end{equation}
with $k\in\klist{0,\ldots,F}$ and $l\in\klist{0,\ldots,T}$ being the frequency and frame index,
$H_{\idxMicSrc}[k]$ is the \DFT/ approximating the \RTF/ of~\eqref{eq:separake:approx_tf},
$\MIC_{\idxSrc}[k,l]$ the \STFT/ of the $\idxSrc$-th source signal, and $\NSE_\idxMic[k,l]$ a term including noise and model mismatch.
It is convenient to group the microphone observations in vector-matrix form,
\begin{equation}
    \MICS[k,l] = \FLTSS[k]\SRCS[k,l] + \NSES[k,l]
    ,
\end{equation}
where $\MICS[k,l],  \NSES[k,l] \in \bbC^{\numMics \times 1}$, $\SRCS[k,l] \in \bbC^{\numSrcs \times 1}$ and $\FLTSS[k,l] \in \bbC^{\numMics \times \numSrcs}$.

\mynewline
Let the squared magnitude of the spectrogram of the $\idxSrc$-th source be $\mP_\idxSrc = \klist{\powerOf{\SRC_\idxSrc}}_{kl} \in \bbR^{F\times T}$.
As depicted in~\cref{fig:separake:nmf_source}, the spectrogram can be modeled as the product of 2 non-negative matrices:
\marginpar{
    \centering
    \includegraphics[width=\linewidth]{separake/nmf_example_source.pdf}
    \captionof{figure}{
        Spectrogram of a sound source signal decomposed into dictionary and activation.
        Image taken from the slides accompanying the paper of this work~\citeonly{scheibler2018separake}.
    }\label{fig:separake:nmf_source}
}
\begin{equation}
    \label{eq:separake:nmf}
    \mP_\idxSrc =  \mD_\idxSrc \mZ_\idxSrc,
\end{equation}
where $\mD_\idxSrc$ is the non-negative \textit{dictionary} whose columns are called \textit{atoms} and can be interpreted as interpreted as spectral templates of the source,
while the latent variables $\mZ_\idxSrc$, called \textit{activations}, indicating when and how this templates are activated.


\section{Multichannel Audio Source Separation by NMF}
\ac{NMF}-based \acf{MASS} can then be cast as an inference problem in which we minimize the error between the observed $\MICS$ over all possible non-negative factorizations \eqref{eq:separake:nmf}.
This normally involves learning the channels, namely the frequency-domain mixing matrices $\FLTSS$.
Instead of learning them, we build the channels based on the prior knowledge of the earliest few echoes.
As introduced in~\cref{sec:application:separation}, the \ac{NMF}-based \ac{MASS} consists in two steps:
the estimation of source's \acf{PSD} via \ac{NMF} and filters and the actual separation via Wiener Filter.

\mynewline
Here we consider two classical, well-understood multi-channel source separation algorithms which, by default, estimate the channels together with sources' dictionaries and activations.
The first algorithm is \NMFdef/ via \MUdef/ and consider only the magnitudes of the transfer functions.
The second one is the multichannel \ac{NMF} via \EMdef/, which instead explicitly models the phases of the mixing filters.
In this work, we considered only the (over)determined case ($\numSrcs \leq \numMics$).
In the following we briefly describe the idea behind the two algorithms.
The reader is invite to refer to the work of~\citeonly{ozerov2010multichannel} for further details.

\subsection{NMF using Multiplicative Updates (MU-NMF)}\label{sec:separake:mu}
\ac{MU} for \ac{NMF} involves the observed magnitude spectrograms only and the updates rules guarantee non-negativity as long as the initialization is non-negative.
This model has been originally proposed by in \citeonly{Lee:2001ti}, however we will consider its formulation as it appear in~\citeonly{ozerov2010multichannel}.
The observed multi-channel squared magnitude spectra are denoted $\mV_\idxMic = \klist{\powerOf{\MIC_\idxMic[k,l]}}_{kl}$ and their non-negative factorizations
\begin{equation}
    \label{eq:mu_nmf}
    \wh{\mV}_\idxMic = \sum_{\idxSrc=1}^{\numSrcs} \diag(\vQ_{\idxMicSrc}) \mD_\idxSrc \mZ_\idxSrc , \quad \idxMic=1,\ldots,\numMics
\end{equation}
\marginpar{
    \centering
    \includegraphics[width=\linewidth]{separake/mu_nmf_signal_model.pdf}
    \captionof{figure}{
        Schematics of the signal model used for \ac{MU}-\ac{NMF}.
        Image taken from the slides accompanying the paper of this work~\citeonly{scheibler2018separake}.
    }\label{fig:separake:nmf_mu}
}
where $\vQ_{\idxMicSrc} = \klist{\powerOf{\FLT_{\idxMicSrc}[k]}}_k$ is the vector of squared magnitudes of the approximate \RTF/ between microphone $\idxMic$ and source $\idxSrc$.

\newthought{The MU rules minimize} the  \textit{divergence}\sidenote{
    The divergence is an asimmetrical ``distance'' measure.
    The Itakura–Saito diverge was proposed to account for perceptum (dis)similarity between spectra.
    Other type of divergence  are used in Audio Source Separation, such as the Kullback-Leibler divergence.
} between the observed spectrogram $\mV_{\idxMic}[k,l]$ and the model $\wh{\mV}_{\idxMic}[k,l]$.
In case of \textit{Itakura-Saito} divergence~\citeonly{fevotte2009nonnegative}, the cost function writes
\begin{equation}
    \scrC_{\mathtt{MU}}(\Theta_{\mathtt{MU}}) = \sum_{\idxSrc k l} \calD_{\mathtt{IS}}(\mV_{\idxMic}[k,l] | \wh{\mV}_{\idxMic}[k,l])
    + \gamma \sum_\idxSrc \kvvbar{\mZ_\idxSrc}_1,
\end{equation}
where $\calD_{\mathtt{IS}}( v | \hat{v} ) = \frac{v}{\hat{v}} - \log \frac{v}{\hat{v}} - 1$ and $\Theta_{\mathtt{MU}} = \{\vQ_{\idxMicSrc}, \set{\mD_\idxSrc, \mZ_\idxSrc}_\idxSrc\}_{\idxMicSrc}$ is the set of parameters.
We add an $\ell_1$-penalty term to promote sparsity in the activations due to the potentially large size of the dictionary~\citeonly{sun2013universal}.

\newthought{The MU rule} for each scalar parameter of interest $\theta$ is obtained by multiplying its value at previous iteration by the ratio of the negative and positive parts of the derivative of the criterion \wrt/ this parameter, namely,
\begin{equation*}
    \theta \gets \theta \frac{\kbracket{\knabla_\theta \scrC_{\mathtt{MU}}(\Theta_{\mathtt{MU}})}_{-}}
                             {\kbracket{\knabla_\theta \scrC_{\mathtt{MU}}(\Theta_{\mathtt{MU}})}_{+}}
\end{equation*}
where $\scrC_{\mathtt{MU}}(\Theta_{\mathtt{MU}}) = {\kbracket{\knabla_\theta \scrC_{\mathtt{MU}}(\Theta_{\mathtt{MU}})}_{+}} - {\kbracket{\knabla_\theta \scrC_{\mathtt{MU}}(\Theta_{\mathtt{MU}})}_{-}}$ and the summands are both nonnegative.
Following the \ac{MU} rule derivations as in \citeauthor{ozerov2010multichannel}, we obtain:
\begin{align}
    \vQ_{\idxMicSrc} &\gets\vQ_{\idxMicSrc} \odot \frac{\kbracket{\wh{\mV}_\idxSrc^{-2} \odot \mV_\idxSrc \odot \kparen{\mZ_\idxSrc \mD_\idxSrc}}\onesVect_{1\times T}}{
                                            \kbracket{\wh{\mV}_\idxSrc^{-1} \odot \kparen{\mZ_\idxSrc \mD_\idxSrc}}\onesVect_{1\times T}}\\
    \mZ_\idxSrc &\gets \mZ_\idxSrc \odot \frac{\sum_\idxMic \ktranspose{(\diag(\vQ_{\idxMicSrc}) \mD_\idxSrc)} \kparen{\mV_\idxSrc \odot \wh{\mV}_\idxSrc^{-2}}}{
                                            \sum_\idxMic \ktranspose{(\diag(\vQ_{\idxMicSrc}) \mD_\idxSrc)} \wh{\mV}_\idxSrc^{-1} + \gamma},\\
    \mD_\idxSrc &\gets \mD_\idxSrc \odot \frac{\sum_\idxMic \ktranspose{ \diag(\vQ_{\idxMicSrc})} \kparen{\mV_\idxSrc \odot \wh{\mV}_\idxSrc^{-2}}\ktranspose{\mZ_\idxSrc}}{
                                            \sum_\idxMic \ktranspose{ \diag(\vQ_{\idxMicSrc})} \wh{\mV}_\idxSrc^{-1} \ktranspose{\mZ_\idxSrc}},
\end{align}
where multiplication $\odot$, power, and division are element-wise and $\onesVect_{1\times T}$ is a vector of $T$ ones.

\subsection{NMF using Expectation Maximization (EM-NMF)}\label{sec:separake:em}
Unlike the \ac{MU} algorithm that independently maximizes the log-likelihood of mixture spectral magnitudes over individual channels, the \ac{EM}-\ac{NMF} maximizes the joint log-likelihood over all complex-valued channels~\citeonly{ozerov2010multichannel}.
Hence, the model takes explicitly into account observed phases.
In this approach, each source $\idxSrc$ is modeled as a complex Gaussian in the form of
\begin{equation}
    \SRC_\idxSrc[k,l] \sim \mathcal{N}_c \kparen{0, (\mD_\idxSrc\mZ_\idxSrc)_{kl}},
\end{equation}
and the magnitude spectrum $\mP_\idxSrc$ of \eqref{eq:separake:nmf} can be understood as the variance of source $\idxSrc$.
This underlying statistical model for the source signal can be used (with little changes) in the \ac{MU}-\ac{NMF} approach.
However, the \ac{MU}-based approach can be applied deterministically, without any statistical assumption on the latent variables.
\\Under this model, and assuming uncorrelated noise, the microphone signals also follow a complex Gaussian distribution with covariance matrix
\begin{equation}
    \mSigma_{\MICS}[k,l] = \FLTSS[k] \mSigma_{\SRCS} [k,l] \khermitian{\FLTSS}[k] + \mSigma_{\NSES}[k,l]
    ,
\end{equation}
where $\mSigma_{\SRCS}$ and $\mSigma_{\NSES}$ are the covariance matrices of the sources and noise, respectively.

\newthought{The EM cost function} corresponds to the negative log-likelihood of the observed signal, that is,
\begin{equation}\label{eq:separake:loglike}
    \scrC_{\mathtt{EM}}(\Theta_{\mathtt{EM}}) = \sum\limits_{kl} \trace\kparen{\MICS[k,l] \khermitian{\MICS[k,l]} \mSigma_{\MICS}^{-1}[k,l]} \\
    + \log\det\mSigma_{\MICS}[k,l].
\end{equation}
where the $\Theta_{\mathtt{EM}} = \set{\FLTSS, \set{\mD_\idxSrc, \mZ_\idxSrc}_\idxSrc, \mSigma_{\NSES}}$ is the set of parameters.

\newthought{The EM algorithm} estimates all the parameters $\Theta$ by alternating between the so-called E-step and M-step.
In a nutshell, one iteration of the E-step consists in computing the \textit{conditional expectation} of the the ``complete'' log likelihood\sidenote{
    The complete data log-likelihood includes both observed variables $\MICS$ and latent variables $\SRCS$} with respect to the current parameter estimates,
and the M-step re-estimates the parameters by maximizing the conditional expectation of the complete log-likelihood.
This quantity can be efficiently minimized using the \ac{EM} algorithm proposed in~\citeonly{ozerov2010multichannel}.
Since adding sparsity priors is not straightforward in the \ac{EM} framework, it was not included in the proposed method.
% \begin{description}
%     \item[E-step:] Conditional expectation \wrt/ the current parameters
%     \begin{align}
%         a &= a\\
%         a &= a\\
%     \end{align}
%     \item[M-step:] Updates of the parameters
%     \begin{align}
%         \mD = \frac{1}{N} \sum_N
%     \end{align}
% \end{description}

% \subsection{Reconstruction of the sources}
% The reconstruction of the image sources is achieved by Wiener filtering, namely
% \begin{equation}
%     \IMG_\idxMicSrc[k,l] = \FLT_{\idxMicSrc}[k] \SRC_\idxSrc[k,l] =
% \end{equation}

\subsection{Supervised NMF with pre-trained Dictionaries}
Pre-trained \textit{dictionaries} are a typical way to informing the \ac{NMF} algorithm, which is sometimes referred to as \textit{supervised \ac{NMF}}.
The idea is to run \ac{NMF} on training sets containing examples from desired sound classes and collect the atoms of the estimated non-negative matrices~\citeonly{schmidt2006single}.
At test phase, these atoms are used as basis vectors for the dictionary matrix (\ie/, $\mD$) and can be used as a good initialization point or kept fixed in the algorithm\sidenote{
In the context of \ac{NMF}-based music transcription applied to piano music, the dictionary can be a collection of spectral templates, each of which is associated to a piano note~\citeonly{muller2015fundamentals}}.
This can be seen as an instance of the problem of \textit{dictionary learning} which also exists in many other research fields.
For audio source separation, this idea has been studied extensively since promising results were obtained, even in single channel scenarios ~\citeonly{smaragdis2009sparse}.
As discussed later in~\cref{subsec:separake:dictionary}, in this work we will use two different dictionaries: one \textit{universal}, and the other \textit{speaker-specific}.


\section{Echo-aware Audio Source Separation}
To evaluate the usefulness of echoes in source separation, we modified the the multi-channel \ac{NMF} framework of \citeauthor{ozerov2010multichannel}~\citeonly{ozerov2010multichannel}.
The knowledge of the echoes is embedded in the model by approximating the entries of mixing matrices with~\eqref{eq:separake:approx_tf}, that is,
\begin{equation}
    \begin{aligned}
        \FLT_{\idxMicSrc}[k] &= \sum_{\idxEch=0}^{\numEchs} \alpha_{\idxMicSrc}^{(\idxEch)} \cste^{-\csti 2 \pi f_k \tau_{\idxMicSrc}^{(\idxEch)}},\\
        \FLTSS[k] &= \kbracket{\FLT_{\idxMicSrc}[k]}_{\idxMicSrc},
    \end{aligned}
\end{equation}
where $f_k = k\Fs/F$ are the discretized frequencies in Hz corresponding to the $k$-th bin in the \DFT/.
\\Futhermore, the early-echo channel model is kept fixed throughout the iterations.
Moreover, instead of updating both sources' dictionaries and activations, we adapted pre-trained dictionaries to better guide the source separation.

\mynewline
Neglecting the reverberation (or working in the far-field anechoic regime) leads to a constant and equal $\vQ_{\idxMicSrc}$ for all $\idxSrc$ and $\idxMic$.
A consequence is that, if we also assume a unique, universal dictionary, namely, $\mD = \mD_j\;\forall j$, then \ac{MU}-\ac{NMF} framework can be simplified.
\\Indeed, \eqref{eq:mu_nmf} becomes the same for all $\idxMic$,
\begin{equation*}
    \wh{\mV}_\idxMic = \sum_{\idxSrc} \mD \mZ_\idxSrc = \mD \sum_\idxSrc \mZ_\idxSrc,
\end{equation*}
so even with the correct atoms identified, we can assign them to any source without changing the value of the cost function.
Therefore, anechoic multi-channel separation with a universal dictionary cannot work well.
This intuitive reasoning is corroborated by numerical experiments in Section \ref{sec:results}.
The problem is overcome by the \ac{EM}-\ac{NMF} algorithm which keeps the channel phase and is thus able to exploit the phase diversity across the array.
Of course, as showed in this work, it is also overcome by using echoes.

\section{Numerical experiments}

We test our hypotheses through computer simulations.
In the following, we describe the simulation setup, dictionary learning protocols, and we discuss the results.

\subsection{Setup}
An array of three microphones arranged on the corners of an equilateral triangle with edge length $\SI{0.3}{\m}$ is placed in the corner of a 3D room with 7 walls.
We select 40 sources at random locations at a distance ranging from $\SI{2.5}{\m}$ to $\SI{4}{\m}$ from the microphone array.
Pairs of sources are chosen so that they are at least $\SI{1}{\m}$ apart.
The floor plan and the locations of microphones are depicted in Figure~\ref{fig:separake:rir_room}\marginpar{
    \centering
    \includegraphics[width=\linewidth]{figures/separake/room_setup}
    \captionof{figure}{
        The simulated scenario.
        Image taken from the paper of this work~\citeonly{scheibler2018separake}
    }
    \label{fig:separake:rir_room}
}.
The scenario is repeated for every two active sources out of the 780 possible pairs.

\mynewline
The sound propagation between sources and microphones is simulated using the
image source model implemented in the \library{pyroomacoustics} Python package~\citeonly{scheibler2017pyroomacoustics}.
The wall absorption factor is set to 0.4, leading to a $\RT$ of approximately 100~ms.\sidenote{
    \itshape
    The resulting \RT{} is short. Realistic values for office and house rooms are typically around 500~ms.
    At the time of the publication we did not investigate different reverberation levels as the focus was on the early echoes.
}
The sampling frequency is set to $\SI{16}{\kHz}$, \STFT/ frame size to $2048$ samples with $50\%$ overlap between frames, and we use a cosine window for analysis and synthesis.
Partial \RTFs/ are then built from the $\numEchs$ nearest image microphones.
The global delay is discarded, and only the relative amplitudes between echoes are kept.

\mynewline
With this setup, we perform three different experiments.
In the first one, we evaluate \ac{MU}-\ac{NMF} with a universal dictionary.
In the other two, we evaluate the performance of \ac{MU}-\ac{NMF} and \ac{EM}-\ac{NMF} with speaker-specific dictionaries.
We vary $\numEchs$ from 1 to 6 and use the following three baseline scenarios:
\begin{enumerate}
\item \textit{anechoic}: anechoic conditions, thus, without echoes in the mixture.
\item \textit{learn}: the \RTFs/ are learned from the data together with the activations as originally proposed~\citeonly{ozerov2010multichannel} and the full reverberation is present in the observed data.
\item \textit{no echoes}: Reverberation is present but ignored (i.e. $\numEchs=0$).
\end{enumerate}
With the universal dictionary, the large number of latent variables warrants the introduction of sparsity-inducing regularization.
The value of the regularization parameter $\gamma$ was chosen by a grid search on a holdout set with the signal-to-distortion ratio ($\SDR$) as the figure of merit \citeonly{vincent2007first} (See ~\cref{tab:separake:gamma}).

\begin{table}
    \begin{sidecaption}[]{
        Value of the regularization parameter $\gamma$ used with the universal dictionary.
        }[tab:separake:gamma]
        \centering
        \small
        \input{tables/separake/params.tex}
    \end{sidecaption}
\end{table}


\subsection{Dictionary Training, Test Set}\label{subsec:separake:dictionary}
First, we introduce a dictionary learned from available training data.
We explore both speaker-specific and universal dictionaries \citeonly{sun2013universal}.
Speaker-specific dictionaries can be beneficial when speakers are known in advance.
Universal dictionary is more versatile but gives a weaker regularization prior.
All dictionaries were trained on samples from the TIMIT corpus~\citeonly{garofolo1993timit} using the \ac{NMF} solver in \library{scikit-learn} Python package~\citeonly{pedregosa2011scikit}.
\cref{fig:separake:dictionaries} illustrates the two different dictionaries used in this work.

\begin{figure}[h]
    \begin{fullwidth}
    \centering
    \subfloat[mu_univ][Universal dictionary]{
        \includegraphics[width=0.48\linewidth]{separake/dict_universal.pdf}
        \label{fig:separake:dict_univ}}
    \hfill
    \subfloat[mu_spkr][Speaker-specific dictionary]{
        \includegraphics[width=0.48\linewidth]{separake/dict_speaker_dep.pdf}
        \label{fig:separake:dict_spk}}
        \caption{Schematic representation of the dictionary protocol used in this work.
        Image taken from the slides accompanying the paper of this work~\citeonly{scheibler2018separake}.}
    \label{fig:separake:dictionaries}
    \end{fullwidth}
\end{figure}


\newthought{Universal Dictionary:} Following the methodology of \citeonly{sun2013universal} we select 25 male and 25 female speakers
and use all available training sentences to form the universal dictionary
$
    \mD = [\mD_1^\mathtt{M}\cdots \mD_{25}^\mathtt{M}\,\mD_{1}^\mathtt{F}\cdots\mD_{25}^\mathtt{F}].
$
The test signals were selected from speakers \emph{and} utterances outside the training set.
The number of latent variables per speaker is 10 so that with STFT frame size of 2048 we have $\mD\in\R^{1025\times500}$.

\newthought{Speaker-Specific Dictionary:}
Two dictionaries were trained on one male and one female speaker.
One utterance per speaker was excluded to be used for testing.
The number of latent variables per speaker was set to $20$.

\subsection{Implementation:}
Authors of \citeonly{ozerov2010multichannel} provide a Matlab implementation\sidenote{
    \href{http://www.irisa.fr/metiss/ozerov/Software/multi_nmf_toolbox.zip}{Multichannel nonnegative matrix factorization toolbox (in Matlab)}
} of \ac{MU}-\ac{NMF} and \ac{EM}-\ac{NMF} methods for stereo separation.
We ported their code to Python and extended it to arbitrary number of input channels.
However this software features some ad-hoc decisions which do not fit our scenario.
Thus, we provide a Python adaptation with the following modifications:
\begin{itemize}
    \item first the original code was restricted to the 2-channel case, i.e.  $\numMics = 2$,
    thus, in order to embrace the specificities of our scenario and for the sake of generalization, we extend it to the multi-channel case, that is $\forall \numMics \geq 1$;
    \item the \ac{MU}-\ac{NMF} was modified to handle sparsity constraint as described in \ref{sec:separake:mu};
    \item since the \ac{EM} method degenerates when zero-valued entries are present in the dictionary matrix, $\mD$, all these entries are initially set to a small constant value of $10^{-6}$;
    \item separation is achieved via \ac{MWF} as described in~\citeonly{ozerov2010multichannel}.
    \item the code was further modified to deal with fixed dictionary and channel model matrices, which are normalized in order to avoid indeterminacy issues \citeonly{ozerov2010multichannel}.
\end{itemize}
Finally, no ``\textit{simulated annealing}'' strategy~\citeonly[Section III.A.6]{ozerov2010multichannel} was used in the final experiments.
In fact in some preliminary and informal investigations we noticed that this yielded better results than using annealing.
In the experiments, the number of iterations for \ac{MU}-\ac{NMF} (\resp/ \ac{EM}-\ac{NMF}) was set to 200 (\resp/ 300).

\subsection{Results}
\label{sec:results}

We evaluate performance in terms of \ac{SDR} and \ac{SIR} as defined in \citeonly{vincent2007first}.
We compute these metrics using the \library{mir\_eval} toolbox~\citeonly{raffel2014mir_eval}.

\mynewline
The distributions of \ac{SDR} and \ac{SIR} for separation using \ac{MU}-\ac{NMF} and a universal dictionary are shown in \cref{fig:separake:mu_univ}, with a summary in \cref{fig:separake:median}.
We use the median performance to compare the results from different algorithms.
First, we confirm that separation fails for flat \RTFs/ (\textit{anechoic} and $\numEchs=0$) with \ac{SIR} at around 0~dB.
Learning the \RTFs/ performs somewhat better in terms of \ac{SIR} than in terms of \ac{SDR}, though both are low.
Introducing approximate \RTFs/ significantly improves performance: the proposed approach outperforms the learned approach even with a single echo.
With up to six echoes, gains are +2~dB \ac{SDR} and +5~dB \ac{SIR}.
Interestingly, with more than one echo, non-negativity and echo priors are already sufficient for achieving good separation, overtaking the $\ell_1$ regularization.


\begin{figure}[t]
    \begin{fullwidth}
    \centering
    \subfloat[mu_univ][MU-NMF, Universal dictionary]{
        \includegraphics[width=0.32\linewidth]{separake/20171025-111558_5ae4058906_near_wall_mu_UnivDict_violin_plot.pdf}
        \label{fig:separake:mu_univ}}
    \hfill
    \subfloat[mu_spkr][MU-NMF, Speaker-specific dictionary]{
        \includegraphics[width=0.32\linewidth]{separake/20171026-192746_360771b8ce_near_wall_mu_SpkrDict_violin_plot.pdf}
        \label{fig:separake:mu_spkr}}
    \hfill
    \subfloat[em_spkr][EM-NMF, Speaker-specific dictionary]{
        \includegraphics[width=0.32\linewidth]{separake/20171027-050909_e9d8c07aef6_near_wall_em_SpkrDict_violin_plot.pdf}
        \label{fig:separake:em_spkr}}
    \caption{Distribution of \ac{SDR} and \ac{SIR} for male and female speakers as a function of the number of echoes included in modeling, and comparison with the three baselines.}
    \label{fig:separake:results}
    \end{fullwidth}
\end{figure}


\begin{figure}[b]
    \begin{sidecaption}[]{
            Summary of the median \ac{SDR} and \ac{SIR} for the different algorithms evaluated.
        }[fig:separake:median]
        \centering
        \small
        \includegraphics[width=\linewidth]{separake/all_medians.pdf}
    \end{sidecaption}
\end{figure}


\mynewline
Separation with speaker-dependent dictionaries is less challenging since we have a stronger prior.
Accordingly, as shown in ~\cref{fig:separake:mu_spkr,fig:separake:median}, \ac{MU}-\ac{NMF} now achieves a certain degree of separation even without the channel information.
The gains from using echoes are smaller, though one echo is still sufficient to match the median performance of learned \RTFs/.
Using an echo, however, results in a smaller variance, while adding more echoes further improves \ac{SDR} (\ac{SIR}) by up to +2~dB (+3~dB).

\mynewline
In the same scenario, \ac{EM}-\ac{NMF} (\cref{fig:separake:em_spkr}) has near-perfect performance on anechoic signals which is expected as the problem is overdetermined.
For \ac{MU}, a single echo suffices to reach the performance of learned \RTFs/ and further improve it.
Moreover, echoes significantly improve separation quality as illustrated by up to 3~dB improvement over \textit{learn}.
It is interesting to note that in all experiments the first three echoes nearly saturate the metrics.
This is good news since higher order echoes are hard to estimate.

% \begin{itemize}
%    \item Universal dictionary scenario: Here, the speakers are unknown.
%    \begin{itemize}
%        \item Because the MU method does not use phase and hence no time delay information across channels, source separation in the anechoic scenario is impossible.
%              Here, we see that using reverberated signals instead with knowledge of only a few echoes significantly improve source separation performance: up to +2dB SDR and +5 dB SIR.
%        \item Using a fixed echo model for channels also outperform learning the channels, even with a single echo.
%        \item Note that EM could not be used for the universal dictionary scenario, because the corresponding model is not designed to handle dictionaries with hundreds of atoms. A sparsity-enforcing Bayesian prior on the estimated activation Z would need to be included, which is not straightforward.
%        \item Increasing the number of known echoes beyond 3 does not bring significant improvement.
%    \end{itemize}
%    \item Speaker-dependent dictionary scenario. This scenario is on the one hand easier because speakers are known and on the other hand harder because much less training data is used, allowing much fewer atoms to represent speech in the dictionaries.
%    \begin{itemize}
%        \item Unsurprisingly, EM performs extremely well in the anechoic setting. This is because perfect knowledge of the complex-valued over-determined mixing filters is then available, through the time differences of arrival. Results would likely be very different in an under-determined setting, where perfect filter knowledge is not enough to separate signals.
%        \item Once again, it is showed that using knowledge of a few echoes significantly improve results with respect to an anechoic model, up to +2dB SDR and +3dB SIR for both methods.
%        \item EM performs slightly better (+1.5 dB) than MU in terms of SIR, suggesting that modeling the phase of mixing filters help.
%        \item In this scenario, knowledge of the echoes starts significantly outperforming the baseline (the learned model) when 3 echoes are known (+3dB SIR).
%        \item Again, increasing the number of known echoes beyond 3 does not bring significant improvement.
%    \end{itemize}
% \end{itemize}

\section{Conclusion}
In this work, we investigated the potential benefit of early echo knowledge for the problem of sound source separation.
Unlike earlier work, instead of fitting an echo model or trying to estimate blindly the acoustic channels,
we investigate the potential of including the properties of known echoes in well established \ac{NMF}-based source separation algorithms.
In particular, we modified the \ac{MU} approach (which considers only spectral magnitudes) and the \ac{EM} (which accounts for complex spectra) by integrating a simple echo model.
Despite its simplicity, such echo model lends itself to a interesting interpretation by revising the \ac{ISM} model:
to each echo corresponds an image microphone (instead of image source as in \ac{ISM}).
It follows that real and image microphones can be considered as microphones arrays with specific directivity pattern.
\\Numerical results shows that echoes seem to play an essential role in magnitude-only algorithms, like the \ac{MU}-\ac{NMF}.
In general, they show that using knowledge of a few echoes significantly improve results with respect to an anechoic model.
This improvement is measured by the standard metrics even when compared to approaches that learn the transfer functions.
\\Finally, this work confirms the potential of including echoes in sound source separation framework.
% To conclude with, does echoes helps sound source separation? The answer is \textit{yes}.

\newthought{Future work} on echo-aware source separation could include:
\begin{itemize}
    \item integrating the blind estimation of the echoes properties, \eg/ using the algorithm \blaster{}, proposed in~\cref{ch:blaster};
    \item including the late reverberation part in the model for the \RTFs/;
    \item experiment with more microphones, more room configurations, more sources on real data, \eg/, using the ones offered by the \dEchorate{} dataset, described in~\cref{ch:dechorate};
    In fact, such dataset was designed and recorded also for echo-aware application. In this sense, the method discussed in this chapter can be perfectly tested on such kind of dataset.
\end{itemize}
\qed
