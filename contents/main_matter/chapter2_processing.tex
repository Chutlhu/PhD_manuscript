\chapter{Elements of Audio Signal Processing}\label{ch:processing}
\openepigraph{Everything in its right place \\
              There are two colours in my head}{Radiohead \textit{Everything In Its Right Place}}
\vspace{-2.5em}
\newthought{Synopsis}\synopsisChProcessing

\mynewline
Unless specified, the notation and definitions presented in this chapter for the audio signal model are excerpted from~\citeauthor{vincent2018audio}'s book \textit{Audio source separation and speech enhancement}.
The material used for illustrating concepts of digital signal processing are taken from standard books on the topics.

\section{Signal model in the time domain}\label{sec:processing:model}
In the previous chapter we formalized the physics that rule the sound propagation from the source to the microphone.
A raw \textit{audio signal} encodes the variation of pressure over time on the microphone membrane.
Mathematically it is denoted as the function
\begin{equation}
    \begin{aligned}
        \tildex : \bbR &\kto \bbR\\
                     t &\kmapsto \tilde{x}(t),
    \end{aligned}
\end{equation}
continuous both in time $t \in \R$ and amplitudes.
\marginpar{
    \centering
    \footnotesize
    \includegraphics[width=\linewidth]{processing/py_processing.pdf}
    \captionof{figure}{Continuous-time signal and its sampled version.}
    \label{fig:processing:sampling}
}

\mynewline
Today signals are typically processed, stored and analyzed by computers as \textit{digital audio signals}.
This corresponds to finite and discrete-time signal $x_\numSample$ obtained by periodically sampling the continuous-time signal $\tilde{x}$ at rate $\Fs\;[\si{\Hz}]$, truncate it to $\numSample$ samples.
\marginpar{
    \footnotesize\itshape
    Strickly speaking, the digital representation of a continuous signal involves sampling and quantization.
    In this thesis we assume the sampled signals are real-valued, ignoring the \textit{quantization} process.}
As common to most measurement models, we assume that the sampling process involves two steps:
first, the impinging signal undergoes an ideal low-pass filter $\lowpassfilter$ with frequency support in $\kintervoc{\sfrac{-\Fs}{2}}{\sfrac{\Fs}{2}}$;
\sidenote{
    The ideal low-pass filter is\\$\lowpassfilter(t) = \sinc(t) = \sin(\pi \Fs t) / (\pi \Fs t)$.
    \\The term sinc stands for \textit{sinus cardinal} and was introduced in~\citeonly{woodward1952information} in 1952,
    in which he said that the function "\textit{occurs so often in Fourier analysis and its applications that it does seem to merit some notation of its own}"
}
then it time-support is regularly discretized, $t = n/\Fs$ for $n \in \Z$.
This is expressed by
\begin{equation}\label{eq:processing:sampling}
    \mic[\idxSample] = \kparen{\lowpassfilter \convCont \tilde{x}}\kparen{\frac{n}{\Fs}} \in \R
    ,
\end{equation}
where $\convCont$ is the continuous-time convolution operator.
This will restrict the frequency support of signal to satisfy the \textit{Nyquist–Shannon sampling theorem} and avoid aliasing effect.
\\Finally, at the end of the discretisation process, the $\tilde{x}(t)$ is represented as the finite time series or a vector,
\begin{equation}
    \mic_{\numSample} \in \R^{\numSample}
    ,
\end{equation}
with entries $\mic_{\numSample}[\idxSample]$ for $n = 0, \ldots, \numSample-1$.
\\The choice of $\Fs$ depends on the application since it is a trade-off between computational power, processing and rendering quality.
Historically the two iconic values are $\SI{44.1}{\kHz}$ for music distribution on CDs and $\SI{8}{\kHz}$ for first-generation speech communication.
Now multiples of $\SI{8}{\kHz}$ are typically used in audio processing: ($16, 48, 96, \SI{128}{\kHz}$).

\mynewline
Audio signals are emitted by sources and are observed, received or recorded by microphones.
A set of microphones is called a microphone \textit{array}, whose signals are sometime referred to as \textit{channels}.
In this thesis, these objects are assumed to have been deployed in a indoor environment, called generically \textit{room}.
Let us provide some taxonomy, through some dichotomies, useful for describing the mixing process later:

\dichotomy{Sources \vs/ Mixtures.} Sound sources emits sounds.
When multiple sources are active at the same time, the sounds that reach our ears or are recorded by microphones are superimposed or \textit{mixed} into a single sound.
This resulting signal is denoted as \textit{mixture}.

\dichotomy{Single-Channel \vs/ Multichannel.} The term \textit{channel}
% \sidenote{%
%     Please note, the term \textit{channel} has also different meaning:
%     it indicates the medium in communication (\eg/ Channel Estimation)
%     and sometimes one of the dimension of the input in machine learning (\eg/ image's channel).
% }
is used here to indicate the output of one microphone or one source.
A \textit{single-channel} signal ($\numMics = 1$) is represented by the scalar $\tilde{\mic}(t) \in \R$,
while a \textit{multichannel} ($\numMics >   1$) is represented by the vector $\tilde{\mics}(t) = \ktranspose{\klist{\tilde{\mic}_1(t), \dots, \tilde{\mic}_{\numMics}(t)}} \in \R^{\numMics}$.

\dichotomy{Point \vs/ Diffuse Sources.} \textit{Point sources} are single and well-defined points in the space emitting single-channel signal.
In certain application, human speakers or the sound emitted by a loudspeaker can be reasonably modeled as in this way.
\textit{Diffuse sources} refers for instance to wind, traffic noise, or large musical instruments, which emit sound in a large region of space.
Their sound cannot be associate to a punctual source, but rather a distributed collection of them.

\dichotomy{Directional \vs/ Onmidirectional.} An \textit{omnidirectional} source (\resp/ receiver) will in principle emit (\resp/ record) sound equally from all directions,
both in time and in frequency.
Although this greatly simplifies the models, it is not true in real scenario, where the physical properties of sources (\resp/ receivers) leads to frequency-dependent \textit{directivity patterns}.
In this thesis we will assume omnidirectional sources and receivers.

\subsection{The mixing process}\label{subsec:processing:mixing}
Let us assume the observed signal has $\numMics$ \textit{channels} indexed by $\idxMic \in \kbrace{1,\dots,\numMics}$.
Let us assume that there are $\numSrcs$ sources indexed by $\idxSrc \in \kbrace{1,\dots,\numSrcs}$.
Each microphone $\idxMic$ and each source $\idxSrc$ have a well defined position in the space, $\positionMicrophone_\idxMic$, $\positionSource_\idxSrc$, respectively.
\\The mixing process describes then the nature of the mixtures.
In order to better formalized it, in source separation (\eg/ in \citeonly{sturmel2012linear}) it is common to use an intermediate representation called \emph{source spatial images}:
$\tilde{\img}_{\idxMicSrc}(t)$ describes the contribution of source $\idxSrc$ to microphone $\idxMic$.
Consequently, the \textit{mixture} $\tilde{\mic}_{\idxSrc}$ is the combination of images associated to the source $j$.
This is illustrated in~\cref{fig:processing:mixing}.
Depending on the ``contribution'' the image describes, the following type of mixture can be defined:

\dichotomy{Natural \vs/ Artificial Mixtures.}
The former refers to microphone mixtures recorded simultaneously the same auditory scene, \eg/ teleconferencing systems or hands-free devices.
By contrast, the latters are created by mixing together different individual, possibly processed, recordings.
This are the typical mixtures used professional music production where the usage of long-chain of audio effects typically ``hide'', willingly or not, the recording environment of the sound sources.

\dichotomy{Instantaneous \vs/ Convolutive Mixtures.}
In the first case, the mixing process boils down to a simple linear combination of the source signals, namely
the mixing filters are just scalar factors.
This is the typical scenario when sources are mixed using a mixing console.
\marginpar{%
    \footnotesize
    \centering
    \begin{tabular}{p{0.33\linewidth}|p{0.66\linewidth}}
    \toprule
    instantaneous   & $\tilde{\img}_{\idxMicSrc} =\alpha_{\idxMicSrc} \tilde{\src}(t) $ \\
    anechoic        & $\tilde{\img}_{\idxMicSrc} =\alpha_{\idxMicSrc} \tilde{\src}_j (t - \tau_{\idxMicSrc})$ \\
    convolutive     & $\tilde{\img}_{\idxMicSrc} =(\tilde{h}_{\idxMicSrc} \convCont \tilde{\src}_\idxSrc) (t)$ \\
    \bottomrule
    \end{tabular}
    \captionof{table}{Taxonomy of linear mixing models for a mixture channel $x_i$,
    sources $\src_j$, impulse response $\tilde{h}_{\idxMicSrc}(t)$, scaling factor $\alpha_{\idxMicSrc}$ and delay $\tau_{\idxMicSrc}$.
    Notice that the anechoic cases is a particular case of the convolutive one, where $\tilde{h}_{\idxMicSrc}(t) = \alpha_{\idxMicSrc}\delta(t - \tau_{\idxMicSrc})$
    }
}
Convolutive mixtures, instead, denote the more general case where the each mixture is the sum of filtered signals.
In between are the \textit{anechoic} mixtures involving the sum of scaled and delayed source signals.
Natural mixtures are convolutive by nature and free-far-field natural recording are well approximated by anechoic mixtures.

\begin{figure}[t]
    \begin{fullwidth}
        \includegraphics[width=\linewidth+\marginparsep]{processing/mixing_process.png}

        \caption{General mixing process, illustrated in the case of $\numSrcs = 3$ sources,
        including three point sources and one diffuse source, and $\numMics = 2$ channels.}

        \label{fig:processing:mixing}
    \end{fullwidth}
\end{figure}

\mynewline
In this thesis, we will particularly focus on natural mixture.
The microphones listen to the propagation of sound in the room and this process is linear (\cfr{\cref{ch:acoustics:sec:wave}}) and time invariant provided a static scenario.
% In this case, the spatialization operation $\spat_\idxSrc(\cdot)$ is expressed by
% collection of convolution with \ac{RIR} $h_{\idxMicSrc}$
% from source $\idxSrc$ to microphone $\idxMic$ and the post-mixing operation $\master(\cdot)$ reduces to the identity:
Therefore, the resulting mixture is the simple summation of the sound images,
which are the collections of convolution between the \acp{RIR} and source signals:
\begin{align}
    \tilde{\img}_{\idxMicSrc}(t) &= \kparen{\tilde{\rir}_{\idxMicSrc} \convCont \tilde{\src}_\idxSrc} (t)     \label{eq:processing:mixing:img} \\
    \tilde{\imgs}_\idxSrc(t)         &= \ktranspose{\klist{\tilde{\img}_{1\idxSrc}(t), \dots, \tilde{\img}_{\numMics\idxSrc}(t)}} \nonumber\\
    \tilde{\mics}(t)                 &= \sum_{\idxSrc=1}^{\numSrcs} \tilde{\imgs}_\idxSrc(t)                    \label{eq:processing:mixing:mix}
    .
\end{align}\marginpar{
    \centering
        \tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt
        \resizebox{\linewidth}{!}{
            \input{figures/processing/mixing_model.tikz}
        }
        \captionof{figure}{Graphical representation of the mixing model~\ref{eq:processing:mixing:mix} for 2 sources and 2 microphones.
        The text is overlapping on purpose.
        }\label{fig:processing:mixingmodel}
}%
Considering the time domain description of the \ac{RIR} derived (and approximated) in the previous chapter,
the time-domain \emph{mixing filters} $\tilde{h}_{\idxMicSrc}( t)$ will be modeled as follows:
\begin{equation}\label{eq:processing:mixing_filter}
    \tilde{h}_{\idxMicSrc}( t) = \sum_{\idxEch=0}^{\numEchs} \frac{\absCoeff_{\idxMicSrc}^{(r)}}{4 \pi \speedOfSound \tau_{\idxMicSrc}^{(r)}}
                          \diracOf{t - \tau_{\idxMicSrc}^{(r)}} + \tilde{\varepsilon}_{\idxMicSrc}(t)
\end{equation}
where $\absCoeff_{\idxMicSrc}^r \in \R$ and $\tau_{\idxMicSrc}^r \in \R$ are the attenuation coefficient and the time delay of the reflection $\idxEch$.
The noise term $\tilde{\varepsilon}_{\idxMicSrc}( t)$ collects later echoes ($\idxEch > \numEchs$) and the tail of the reverberation.
We do not assume $\tilde{\varepsilon}_{\idxMicSrc}( t)$ to be known.

\subsection{Noise, interferer and errors}
% \openepigraph{%
%     \emph{Noise} is a general term for unwanted (and, in general, unknown) modifications that a signal may suffer during capture, storage, transmission, processing, or conversion
% }{V. Tuzlukov, \textit{Signal processing noise}}
In~\cref{eq:processing:mixing:mix} no noise is included:
all the sources are treated in the same way, including \textit{target}, \textit{interfering} and \textit{noise} sources.
While the definition of target sound source is quite self-explanatory (and will denoted by default as the first source, that is $j = 1$), the term interfer and noise depends on the specific use case, problem, application, and research field.
Notice that in~\cref{eq:processing:mixing_filter} a noise term is added to gather unknown quantities.
\begin{quote}
    \textit{Noise is a general term for unwanted (and, in general, unknown) modifications that a signal may suffer during capture, storage, transmission, processing, or conversion}
    \citeonly{tuzlukov2018signal}.
\end{quote}
Therefore, we will define and use the following type of noises:
\marginpar{
    \centering
        \resizebox{\linewidth}{!}{
            \input{figures/processing/noise_model.tikz}
        }
        \captionof{figure}{Graphical representation of the mixing model~\eqref{eq:processing:mixing:mix}:
        $s_2(t)$ is the \textit{interferer},
        $\boldsymbol{n}( t)$ contributes to the \textit{diffuse noise field}, and
        $\boldsymbol{\varepsilon }( t)$ model acquisition and modeling errors.
        \textit{With proper prior knowledge, it can be understood}.
        }\label{fig:processing:noise_mixing}
}
\newthought{Interfers} identifies undesired sources with properties similar to the target source.
For instance, a concurrent speech source for speech application or concurrent music instrument in case of music.
Later, in this thesis the interfer sources will be denoted as additional sources indexed by $j > 1$.

\newthought{Noise} collects all the remaining effects, typically nonspeech sources.
Moreover we will make a further distinction between the followings.
\begin{itemize}
    \item
    \textit{Diffuse Noise Field} describes the background diffuse sources present in the auditory scene, \eg/ car noise, indistinct talking or winds.
    It can be recorded or approximated as \AWGN/ with a specific spatial description as described in~\citeonly{habets2007generating}.
    \item
    \textit{Measurement and Model Noise} accounts for general residual mis-modeling error.
    In this manuscript, it will denoted as $\tilde{\varepsilon}_{\idxMicSrc}( t)$ and will be modeled as \AWGN/.
    Examples of this term are the error due to approximation of the \ac{RIR} with the \acf{ISM} or sensor noise, respectively.
\end{itemize}

\mynewline
By making the noisy terms explicit, the mixing model in~\cref{eq:processing:mixing:img,eq:processing:mixing:mix} writes:
\begin{align}\label{eq:acoustics:mixing:align1}
    \tilde{\img}_{\idxMicSrc}(t) &=  \kparen{\tilde{\rir}_{\idxMicSrc} \convCont \tilde{\src}_\idxSrc} (t) +  \tilde{\varepsilon}_{\idxMicSrc}(t)\\
    \tilde{\imgs}_\idxSrc(t)         &= \ktranspose{\klist{\tilde{\img}_{1\idxSrc}(t), \dots, \tilde{\img}_{\numMics\idxSrc}(t)}} \nonumber\\
    \tilde{\mics}(t)                 &= \sum_{\idxSrc=1}^{\numSrcs} \tilde{\imgs}_\idxSrc(t) + \tilde{\bsn}( t)
\end{align}
Note that the noise term $\tilde{\varepsilon}_{\idxMicSrc}$ of~\cref{eq:acoustics:mixing:align1} does not correspond to the one of \cref{eq:processing:mixing_filter}, but includes it.
Thus, as not to weigh down the notation, we will use the same notation for both.

\section{Signal model in the spectral domain}\label{sec:processing:domains}\marginpar{%
    \footnotesize\itshape
    The frequency analysis was introduced by Joseph Fourier in his work on the heat equation~\citeonly{fourier1822theorie}.
    His mathematical tool, named later \textit{Fourier Decomposition},
    aims at approximating any signal by a sum of sine and cosine waves.}%
The frequency, or spectral, representation is probably the most famous signal representation used in signal processing:
Speech and music signals naturally exhibit pseudo-periodic behaviors (on well chosen time scale).
In this domain, they are described as combination of sinusoids as function of their frequencies.
% https://tex.stackexchange.com/questions/127375/replicate-the-fourier-transform-time-frequency-domains-correspondence-illustrati
\\This operation is achieved by the operator \FTdef/, $\fourierTrans{}$, which projects a continuous-time-domain square-integrable signal $\tilde{x}$ onto a space spanned by continuous-frequency complex exponentials (see~\cref{fig:processing:fourier}):
\begin{equation}\label{eq:processing:ft}
    \tilde{X}(f) = (\fourierTrans{\tilde{x}})(f) =
        \int_{-\infty}^{+\infty}
        \tilde{x}(t)
        \cste^{-\csti 2 \pi f t}
        \,\kdiff{t} \quad \in \bbC
    ,
\end{equation}
where $f \in \bbR$ are the \textit{natural frequency} in $\si{\Hz}$ and $\csti$ is the imaginary unit.
\marginpar{
    \centering
    \resizebox{\linewidth}{!}{
        \input{figures/processing/time2freq.tikz}
    }
    \captionof{figure}{
        A signal resolved into its Fourier series:
        a linear combination of sines and cosines
        represented as peaks in the frequency domain.
        Adapted from the \href{https://tex.stackexchange.com/questions/127375/replicate-the-fourier-transform-time-frequency-domains-correspondence-illustrati}{online tikz example}.
    }\label{fig:processing:fourier}
}
\\Apart from providing a space where an audio signal reveals its harmonic structure, the Fourier transform benefits from two fundamental properties:
it is linear and it converts time-domain convolution into element-wise product.
\\First, linearity allows to write~\cref{eq:processing:mixing:mix} simply as:
\begin{equation}\label{eq:processing:ft:mix}
    \tilde{\mics}( t) = \sum_{\idxSrc=1}^{\numSrcs} \tilde{\imgs}_\idxSrc( t)
    \mathspace\overset{\fourierTrans{}}{\kto}\mathspace
    \tilde{\MICS}( f) = \sum_{\idxSrc=1}^{\numSrcs} \tilde{\IMGS}_\idxSrc( f)
\end{equation}
Secondly, by the \textit{convolution theorem}, the source spatial images in~\cref{eq:processing:mixing:img} writes as:
\begin{equation}\label{eq:processing:conv}
    \tilde{\img}_{\idxMicSrc}( t) =  \kparen{\tilde{\rir}_{\idxMicSrc} \convCont \tilde{\src}_\idxSrc} ( t)
    \mathspace\overset{\fourierTrans{}}{\kto}\mathspace
    \tilde{\IMG}_{\idxMicSrc}( f) =  \tilde{H}_{\idxMicSrc}( f) \tilde{S}_\idxSrc( f)
    .
\end{equation}

\mynewline
As discussed in~\cref{ch:acoustics}, assuming a pure echo model, the \FT/ of \ac{RIR}, \aka/ the \RTFdef/, can be computed exactly in closed-form as
\begin{equation}\label{eq:processing:rir:ft}
    \tilde{H}_{\idxMicSrc}( f) = \sum_{\idxEch=0}^{\numEchs}
                                 \frac{\absCoeff_{\idxMicSrc}^{(r)}}{4 \pi \speedOfSound \tau_{\idxMicSrc}^{(r)}}
                                 \cste^{- \csti 2 \pi f \tau_{\idxMicSrc}^{(r)}}
    .
\end{equation}
In practice, the filters $\tilde{h}_{\idxMicSrc}$ are neither available in the continuous time domain nor in the continuous frequency domain directly.
They must be estimated from the observation of the discrete-time mixtures $x_i[n]$, therefore, after the convolution with a source and the measurement process.
In practice, we don't have access to continuous signals, neither in time nor in frequency domain.
Every signal the microphones capture and the related spectra are represented by finite- and discrete-time signals for which the properties~\eqref{eq:processing:conv} are valid only with some precautions.
% That gives rise to the field of digital signal processing.

\subsection{Discrete time and frequency domains}\label{subsec:processing:dtft_dft}
In case of discrete-time signal, $x[n]$ with $n \in \bbZ$ and frequency support in $[\sfrac{-\Fs}{2}, \sfrac{-\Fs}{2}]$, its spectral representation is given by the (forward) \DTFTdef/, $\dtft$:
\begin{equation}\label{eq:processing:dtft}
    \tildeX_{\Fs}(f) = (\dtft{x})(f) =
    \sum_{n = -\infty}^{+\infty}
    x[ n]
    \cste^{-\csti 2 \pi f n / Fs}
    ,
\end{equation}
which is a continuous function of $f$ with period $\Fs$.
Notice that the term ``discrete-time'' refers to the fact that the transform operates on discrete signal.
In case of uniformly spaced samples, it produces a function of continuous frequency that is a periodic summation of the continuous Fourier transform of the original continuous function.
Under certain theoretical conditions, described by the \textit{sampling theorem}, both the original continuous signal $\tildex$ and its sampled version $x$ can be recovered perfectly from the \DTFT/.
\\The analogous convolution theorem for discrete-time domain signals $\src[n]$ and $\rir[n]$ is
\begin{equation}\label{eq:processing:conv_dtft}
    \img[n] = \kparen{\rir \convCont \tilde{\src}} [ n]
    \mathspace\overset{\dtft}{\kto}\mathspace
    \tilde{\IMG}_{\Fs}( f) =  \tilde{\FLT}_{\Fs}( f) \tilde{\SRC}_{\Fs}( f)
    ,
\end{equation}
where the dependency on the source and microphone index have been omitted for clarity.
\\The \DTFT/ itself is a continuous function of frequency which require infinite discrete value to be computed.
For these two reason, it is not accessible in practice or computed in the digital domain.
Therefore the following representation is used instead.

\mynewline
For a discrete- and finite-time signal $x_N$, the spectral representation is given by its (forward) \DFTdef/
\sidenote{
    This can be intrepreded as the projection onto the space spanned by a finite number of complex exponentials.
},
$\discreteFT{}$:
\begin{equation}\label{eq:processing:dft}
    X_F[ k] = (\discreteFT{x_N})[k] =
    \sum_{n = 0}^{N - 1}
    x_N[ n]
    \cste^{-\csti2\pi k n / F}
    .
\end{equation}
where $k \in \kintervcc{0}{F - 1}$ is the discrete \textit{frequency bin} and $F$ is the total number of bins.
Again we use the subscript $F$ and the brackets $[k]$ to stress the finite and discrete frequency support of the $\DFT/$.
Note that now the sampling frequency term disappeared from the definition of the operators and, in general, operates directly on ``sequences''.
Therefore its link with the continuous \ac{FT} for continuous-time domain signal is not trivial and it will be discussed in the following section.
\\Similarly to the \ac{FT} and the \ac{DTFT}, the convolution theorem can be defined for the \ac{DFT} as well.
However, it comes with following important modification.
Let be two finite- and discrete-time domain \textit{periodic} signals $\dddot{\src}_N$ and $\dddot{\rir}_N$ of period $N$.
Here $\dddot{\cdot}$ is used to denote periodicity.
Let be $\SRC_F$ and $\FLT_F$ their \ac{DFT}, respectively, then the convolution theorem for the \ac{DFT} writes
\begin{equation}\label{eq:processing:conv_dft}
    \dddot{\img}_N[n] = \kparen{\dddot{\rir}_N \circledast \dddot{\src}_N} [ n]
    \mathspace\overset{\discreteFT}{\kto}\mathspace
    \IMG_{F}[k] =  \FLT_{F}[k] \SRC_{F}[k]
    ,
\end{equation}
where $\convCir$ denotes the \textit{circular convolution}\sidenote{
    The finite-time circular convolution for two vectors $u_N,v_N\in\bbR^N$ is the
    $\dddot{q}_N = (u_N \convCir v_N)[n] = \sum_{m=0}^{N-1} u_N[m] \dddot{v}_N[n-m]$,
    where $\dddot{v}_N$ is the periodic version of $v_N$ and $\dddot{{q}_N}$ is periodic.
}. Note that now the results of the convolution, $\dddot{\img}_N[n]$, is periodic with period $N$.

\subsection{The DFT as approximation of the FT}\label{subsec:processing:ftapprox}
An important application of the \DFT/ is to approximate numerically the \FT/.
As mentioned at the beginning of the chapter, with the discretisation process the continuous signal is periodically sampled, low-passed and finally truncated.
It can be proved that sampling in the time domain corresponds to periodizing the signal spectrum with period equal to the sampling frequency.
In order to avoid aliasing artifact, it is common to limit the signal bandwidth before the sampling (and using the lowpass filter in~\cref{eq:processing:sampling}).
By assuming sampling at rate $\Fs$, in the continuous-frequency domain the spectrum $\tilde{X}(f)$ is repeated every intervals of size $\Fs\,\si{\Hz}$ .
By further assuming that the signal undergoes an ideal low-pass filter, no spectral leakage is presents between each repetition.

\mynewline
So far, the sampled time domain signal, $x[n]$, is mapped to the continuous frequency domain $\tilde{X}(f)$.
This particular case of the \FT/ is called \DTFTdef/ and it is denote with $\tilde{X}_{\Fs}[ k]$.
\begin{equation}\label{eq:processing:ft2dtft}
    \begin{aligned}
        \tilde{X}(f) =
        \int_{-\infty}^{+\infty}
        \tilde{x}(t)
        \cste^{-\csti 2 \pi f t}
        \,\kdiff{t}
        \quad \rightarrow \quad
        \tilde{X}_{\Fs}( f) =
        \sum_{n = -\infty}^{\infty}
        x[ n]
        \cste^{-\csti2\pi f \frac{n}{\Fs}}
    \end{aligned}
    .
\end{equation}
Here the continuous integral the \FT/ is approximated by Riemann sum over the discrete points $n \in \Z$:
To be more rigorous, when computing a Riemann sum approximation, the length of the discretisation interval multiply the summation.
In our application, this quantity always set to $\Fs$ and for readability reason such term is dropped.
The quality of this approximation \wrt/ the original continuous spectrum is regulated by the choice of $\Fs$: the higher $\Fs$, the better the approximation.
The lower bound to the possible value $\Fs$ is the results known as the Nyquist–Shannon's sampling theorem.

\mynewline
Furthermore, we consider only the finite sequence $x_N$ consisting of $N$ samples.
This would reduce the summation ranges the right part of~\cref{eq:processing:ft2dtft}.
Instead, we can keep the infinite summation by multiplying the sampled signal by a discrete-time window function ${w}$ selecting the non-zero porting of $x$, $x_N[n] = {w}[n]x[n]$.
By the \textit{convolution theorem}, the multiplication in the time domain translates in a convolution between the corresponding spectra.
As a consequence, the spectrum of the truncated signal is distorted by the spectrum of the window function.
In math,
\begin{equation}\label{eq:processing:dft2dtft}
    \tilde{X}_{\Fs}( f) =
    \sum_{n = -\infty}^{\infty}
    x[ n] {w}[ n]
    \cste^{-\csti2\pi f \frac{n}{\Fs}}
    \quad \leftrightarrow \quad
    \tilde{X}_N(f) =
    \sum_{n=0}^{N-1}
    x_N[n]
    \cste^{-\csti2\pi f \frac{n}{\Fs}}
    .
\end{equation}
By the convolution theorem, we have that
\begin{equation}
    x_N[n] = x[ n] {w}[ n]
    \quad \leftrightarrow \quad
    \tilde{X}_{N}(f) = (\tilde{X}_{\Fs} \convCont \tilde{W}_{\Fs})(f)
\end{equation}
where $\tilde{W}_{\Fs}$ is the \DTFT/ of the sampled window function ${w}[n]$.
\\Assuming the window function to be an ideal rectangular function, its \DTFT/ is a ideal low-pass filter, which acts on the original spectrum as a smoothing function.
As a consequence, the quality of this approximation is then based on the spectral leakage of the chosen window function, $w[n]$.
As a rule of thumb, here the longer the segment, the better the approximation%
\sidenote{When short excerpt are considered instead (\eg/ in case of the \STFTdef/), particular types of window function are used but their analysis are out of the scope of this thesis.}

\mynewline
Finally, we cannot access the \DTFT/ directly because that involves an infinite number of frequencies $f \in \R$.
Therefore, taking $F$ uniformly-spaced frequency $f_k\in\R$ as in~\cref{eq:processing:fk}, we finally obtain the \DFT/ as in~\cref{eq:processing:dft}, that is
\begin{equation}
    \tilde{X}_N(f_k) =
    \sum_{n=0}^{N-1}
    x_N[n]
    \cste^{-\csti2\pi f \frac{n}{\Fs}}
    \quad \leftrightarrow \quad
    X_F[k] =
    \sum_{n=0}^{N-1}
    x_N[n]
    \cste^{-\csti2\pi k n / F}
    .
\end{equation}
The natural frequency $f_k$ in $\si{\Hz}$ corresponding to the $k$-th frequency bin can be then computed as
\begin{equation}\label{eq:processing:fk}
    f_k = \frac{k}{F}\Fs
    .
\end{equation}
Notice that $\Fs$ term disappeared in the right part of the equation above as it cancels out when using~\cref{eq:processing:fk}.
By increasing $F$, we can sample more densely $X_F[k]$ which leads to a better approximation to $\tilde{X}_N$.
However this does not eliminate the distortion of the previous steps, due to $\tilde{W}_{\Fs}$.

\mynewline
Again, we sampled a domain. Thus, according to the defined sampling process, this involve using a ideal low-pass filter.
This filter acts now on the discrete spectrum, smoothing it and limiting the support of its transformation in the dual domain.
Therefore, the inverse \DFT/ of $X_F[k]$ is not properly $x_N[n]$, but its periodic version repeated every $F$ samples.
In fact, sampling in one of the two domain is equivalent to a periodization in the other domain while truncating lead to convolving with a window function.
Moreover, the chain of operation (sampling in time and truncation in time and sampling in frequencies) are valid in both way.
Thus one can arbitrarily first sample and truncate frequency domain and finally sample in time.
The only difference is in the interpretation of the windowing function, which in one case smooth the spectrum and in the other smooth the signal.
All this relation and approximation that connects the \FT/ to the \DFT/ are well explained in this explanatory material
{\href{https://krasjet.com/rnd.wlk/poisson.pdf}{Poisson Summation Formula, Revisited \ExternalLink}} or in many classic books of digital signal processing, such as~\citeonly{oppenheim1987signals}.

% Through the use of microphones and computervcs, our window into the continuous physical world of acoustics is narrow and discrete, both it time and in frequency.

\subsection{Signal models in the discrete Fourier domain}\label{sec:processing:fouriermodel}
Conscious of the above approximations, we can now rewrite our signal model for the discrete case.
Hereafter we will always consider finite-length sequences and the index $N$ will be dropped to lighten the notation.
\\The \DFT/ is linear, so the discrete version of \cref{eq:processing:ft:mix} becomes
\begin{equation}\label{eq:processing:dft:mix}
    {\mics}[ n] = \sum_{\idxSrc=1}^{\numSrcs} {\imgs}_\idxSrc[ n]
    \mathspace\overset{\discreteFT}{\kto}\mathspace
    {\MICS}[ k] = \sum_{\idxSrc=1}^{\numSrcs} {\IMGS}_\idxSrc[ k]
\end{equation}
Secondly, by using na\"ively the discrete convolution theorem, one could translate \cref{eq:processing:mixing:img} as
\begin{equation}\label{eq:processing:discreteModel}
    {\img}_{\idxMicSrc}[n] = ({\rir}_{\idxMicSrc} \convDis {\src})[n]
    \mathspace\overset{\discreteFT{}}{\kto}\mathspace
    {C}_{\idxMicSrc}[k] = {H}_{\idxMicSrc}[k] {S}[k]
    ,
\end{equation}
where $\convDis$ is the finite-time linear convolution operator\sidenote{
    The finite-time linear convolution for two vectors $u\in\bbR^L$ and $v\in\bbR^D$ is
    \\$(u \convDis v)[n] = \sum_{l=0}^{L-1} u[l] v[L-1+n-l]$ for $n = 0, \cdots, D-L$
    .
}. In general, this relation is wrong and the following paragraph will provide further insight on why is used in practice.
\\The filter ${H}_{\idxMicSrc}[k]$ is the \DFT/ of the room impulse response.
As mentioned in the~\cref{subsec:processing:ftapprox}, this just approximates the true \RTF/ defined in the previous chapter.
We recall that two approximation are involved: one concern that we model only a finite number of specular reflections; one concern sampling its \FT/.
Nevertheless, we can use the closed-form frequency-domain echo model~\cref{eq:acoustics:ims:freq} to write the \RTF/'s \DFT/ in closed-form by taking equally spaced frequencies, that is
\begin{equation}\label{eq:processing:discreteModel:rir}
    {H}_{\idxMicSrc}[ k] = \sum_{\idxEch=0}^{\numEchs}
                \frac{\absCoeff_{\idxMicSrc}^r}{4 \pi \speedOfSound \tau_{\idxMicSrc}^r}
                \cste^{- \csti 2 \pi k \Fs \tau_{\idxMicSrc}^r / F}
    .
\end{equation}

\newthought{Although used in practice}, the model~\eqref{eq:processing:discreteModel} makes use of other approximations that are worth presenting.
In particular, the work by~\citeonly{tukuljac2018mulan} properly discuss them in the context of the echo estimation problem.
The paper mention three approximations, which are depicted in the following diagram.
\begin{figure}[!h]
    \begin{fullwidth}
    \centering
    \input{figures/processing/approx_diagram.tikz}
    \end{fullwidth}
\end{figure}
The diagram shows a chain of operators (sampling and transforms) with provable and approximated equivalences that lead to~\cref{eq:processing:discreteModel} used in practice.
In order,
\begin{enumerate}[label=(\roman*)]
    \item\label{it:processing:dft:approx1}
    In~\citeonly[Proposition 2]{van2001gaussian}\sidenote{%
        Even if this is a classic result and it is discussed is classical book, however the provided reference concisely discusses it.
    } the authors show that if the signal $\tilde{\src}(t)$ is band-limited by $\Fs$,
    then sampling the continuous convolution is \textit{exactly} equivalent to \textit{linearly convolving} the infinite discrete signal ${\src}[n]$ and the discrete and low-passed version of the filter.
    However, while the source signal is band-limited by nature, the true \ac{RIR} $\tilde{h}^*(t)$ is not (in fact the \ac{RIR} is modeled as a finite summation of spikes, which has spectrum with infinite support).
    Thus, the first approximation~\ref{it:processing:dft:approx1} considers ${h}[n] \approx (\lowpassfilter \convDis \tilde{h}^*)[n]$, in words we assume that the filter is band-limited by $\pm \sfrac{\Fs}{2}$.

    \item\label{it:processing:dft:approx2}
    As introduced in~\cref{subsec:processing:dtft_dft}, the discrete-time convolution theorem applies to the \textit{circular convolution} involving periodic signals.
    This can be approximated by the \textit{linear convolution}, that is $(h \circledast \hats)[n] \approx (h \convDis \hats)[n]$ under some assumption.
    In particular such an approximation is reasonably good when many samples are available and when one of the two signals is quasi-periodic, which are typical cases for audio signals.

    \item\label{it:processing:dft:approx3}
    The third approximation regards the closed-form of the \RTF/, $\tildeH^*(f)$.
    Here we make two strong assumptions:
    first, the \RTF/ follows the echo model discussed in~\cref{eq:acoustics:ims:freq};
    second, we used its closed-from \DTFT/ (\cref{eq:processing:discreteModel:rir}) at some frequency $f_k$ as its \DFT/.
    In order to be computed exactly, this would require infinitely many samples and unlimited frequency support, which are not possible in practice.
\end{enumerate}
Nevertheless, it is important to notice that approximations \ref{it:processing:dft:approx2} and \ref{it:processing:dft:approx3} become arbitrarily precise as the number of samples $N$ grows to infinity.
First, the \DFT/ approaches the \DTFT/ as $N\to\infty$ due to their definition (see~\cref{subsec:processing:dtft_dft}).
Second, while the convolution theorem is exact for the \DTFT/, for the \DFT/ it is only for  considering the circular convolution on a finite-time signal (or, equivalently, considering periodic signals).
Therefore, it is easy to notice that that when the period becomes infinite, the circular convolution and the linear convolution become arbitrarily close.
In the approximation \ref{it:processing:dft:approx1}, limiting the \ac{RIR}'s spectrum with an ideal lowpass filter introduces smearing in the time-domain.
\citeauthor{tukuljac2018mulan} made an important observation in this regard:
even if infinite number of samples are available, after the measurement process, the discrete-time filter ${\rir}[n]$ consists of infinite-length decimated combinations of sinc functions.
In the context of this thesis, this observation tell us that even in ideal conditions, that is without noise, possibly knowing the transmitted signal, and processing infinitely many samples, the exact estimation of the echo properties of the \ac{RIR} is a challenging task itself.
This is a fundamental difference between \ac{RIR} estimation and estimating the time of arrivals of the early echoes.
Note, for instance, that we wrote the echo model only in the continuous-time domain or with its closed-form form discrete frequencies.
The discrete-time domain was avoided on purpose since the echoes' arrival time are naturally off the sampling grid, namely they are not integer multiples of $\Fs$.
This fundamental limitation is subject of research in the field of \textit{super-resolution} and it will discussed in~\cref{ch:blaster}.

\mynewline
While the raw audio signal encodes the amplitude of a sound as a function of time, its spectrum represents it as a function of frequency.
In order to jointly account for both temporal and spectral characteristic, joint time-frequency representations are used.

\subsection{Time-Frequency domain representation}\label{subsec:processing:stft}
\TFdef/ representations aim to jointly describe the signal in the time and frequency domains.
Instead of considering the entire signal, the main idea is to consider only a small section of the signal.
To this end, one fixes a so-called \textit{window} function, $w_N[n]$, which is nonzero for only a period of time $\winsize$ shorter than the entire signal length, $\winsize \ll N$.
This function iteratively shifts and multiplies the original signal, producing consecutive \textit{frames}.
Finally, the frequency information are extracted independently from each frame.
The choice of a window function $w[n]$ depends on the application since its contribution reflects in the \TF/ representation together with the
one of the signal.

\mynewline
The discrete \ac{STFT}\marginpar{\footnotesize%
The \ac{STFT} was introduced by Dennis Gabor in 1946, the person behind Holography.
}
is the most commonly used \TF/-representation in audio signal processing.
This representation encodes the time-varying spectra into a matrix $X[k,l] \in \bbC^{F,T}$ with frequency index $k$ and time frame index $l$.
More formally, the process to compute the complex \ac{STFT} coefficients is given by
\begin{equation}\label{eq:processing:stft}
    X[k, l]  = \sum_{n=0}^{\winsize-1} w[n] x[n + l \hopsize] \cste^{- \csti 2 \pi k n / F} \mathspace\in\bbC
\end{equation}
where $\winsize$ is the window length and $\hopsize$ is the \textit{hop size} which specifies how much the window needs to be shifted across the signal.
Equivalently, \cref{eq:processing:stft} can be expressed as \DFT/s of windowed frames, $X[k, l] = \discreteFT{x[n,l]}$ where $x[n,l] = x[n + l \hopsize] w[n]$.
\\Since each \ac{STFT} coefficient $X[k, l]$ lives in the complex space $\bbC$, the squared magnitude of the \ac{STFT}, $\powerOf{X[k,l]}$ is
commonly used for visualization and for processing.\marginpar{
    \centering
    \footnotesize
    \includegraphics[width=\linewidth]{processing/py-speech_spectrogram.pdf}
    \captionof{figure}{STFT spectrogram of an example speech signal. Higher energies are illustrated with darker colors.}
    \label{fig:processing:spectrogram}
} The resulting two-dimensional representation is called (log) power \textit{spectrogram}.\marginpar{%
    \footnotesize\itshape
    For more mathematical detailed description on \DFT/ and \ac{STFT} can be found in \citeonly{oppenheim1987signals}.
    For a audio-processing-oriented and music-processing-oriented explanation please refer to Chapter 2 of \citeonly{vincent2018audio} (Chapter2) and Chapter 2 of \citeonly{muller2015fundamentals}, respectively.
} It can be visualized by means of a two-dimensional image, whose axes represent time frames and frequency bins.
In this image, the (log) value $\powerOf{X[k,l]}$ is represented by the intensity or color in the image at the coordinate $[k,l]$.
Throughout this work processing will be typically conducted in the \ac{STFT} domain, unless specified.
This is a common approach in the audio signal processing community, but it is not the only one:
many algorithm are designed directly in the time domain or in alternatives \TF/ representation, \eg/, Mel-Scale, Filter-Banks.


\mynewline
As discussed~\citeonly{vincent2018audio}, the \ac{STFT} has the following useful properties for audio processing:
\begin{itemize}
    \item the frequency $f_k$ is a linear function of the frequency bin $k$;
    \item the resulting matrix allows the computation of the phase $\phaseOf{X[k,l]}$, the magnitude $\magnitudeOf{X[k,l]}$ and the power $\powerOf{X[k,l]}$;
    \item the \DFT/ can be efficienciently computed with the \FFT/ algorithm;
    \item the process to invert the \ac{STFT} is relatively easy (using the inverse of the \DFT/, overlap-add method with proper window functions);
    \item the \ac{STFT} inherits the linearity and convolution property of the \DFT/ under some condition about the length of the signals.
\end{itemize}

% \begin{figure}[t]
%     \begin{fullwidth}
%     \centering
%         \input{figures/processing/diagram_stft.tikz}
%         \caption{Illustration of the \ac{STFT}.}
%     \end{fullwidth}
% \end{figure}


\subsection{The final model}\label{subsec:processing:model:stft}
The model \eqref{eq:processing:discreteModel} shows how in practice the \acp{RIR} are treated in the frequency-domain.
However this does not generalize straightforwardly to the time-frequency domain:
it depends on the length of the filter \wrt/ to the length of the analysis window on of the \ac{STFT}.
To circumvent this issue, the \textit{convolutional STFT}\sidenote{%
It translates the time-domain convolution into inter-frame and inter-band convolutions, rather than pointwise multiplication of Fourier transforms.
} for arbitrary window functions have been proposed~\citeonly{gilloire1992adaptive}.
Although it leads to better approximation than the one of product, it is computationally and memory intensive.
The exact effect of the time-domain convolution in the \ac{STFT} would be modeled by cross-band filters performing a double convolution along time and frequency axes.
However, these models are not investigated.

\mynewline
In this thesis, we will consider moderated reverberant scenario.
This reflects in long room impulse responses, which would require to long analysis window.
Instead, as we are interested in only the early echoes, we can consider filters shorter than typical window length used in speech processing (\eg/ 64 ms).
In the literature, this is known as the \textit{narrowband approximation}, namely the time-domain filtering can be approximated by complex-valued multiplication in each time-frequency bin $[l,k]$:
\begin{equation}\label{eq:processing:narrow}
    \IMGS_j[l,k] \approx {\FLTS}[k] \SRC_j[l,k]
    ,
\end{equation}
where the ${\FLTS}_j[k] = \ktranspose{\klist{{\flt}_{1j}[k], \cdots, {\flt}_{Ij}[k]}}$ is the $I \times 1$ vector of the \RTF/s for source $j$.
While this is a resonable approximation for short filters, it gets worsens as the filter length grows.

It is sometimes practical to concatenate all these vectors into an $I \times J$ matrix ${\FLTSS}[k] = \klist{\FLTS_1(f),\cdots,\FLTS_J(f)}$ called \textit{mixing matrix}.

\mynewline
With the above notation and considerations, mixing process including noise terms can be written in the \ac{STFT} domain compactly as:
\begin{equation}\label{eq:processing:model:stfs}
    \MICS[l,k] = \FLTSS[k] \SRCS[l,k] + \bsU[l,k]
\end{equation}
where $\bsU[l,k] = \bsN[l,k] + \boldsymbol{\varepsilon}(l,k)$ includes the contribution of both diffuse noise sources, modeling and measurement errors.

% The above equation can be seen as a \textit{deterministic} parametrization of the mixing process and it does not consider a statistical description
% of the reverberation. In fact it the filter

% \newthoughtpar{The spatial quadratic STFT transform}
% \begin{equation}
%     \cov_{\mics}[l,k] = \bbE \klist{\mics[l,k]\khermitian{\mics[l,k]}}
% \end{equation}

\section{Other (room) impulse response spectral models}\label{sec:processing:rirmodels}
\acp{RIR} are complicated quantities to model, compute and estimate.
The representations of the \ac{RIR} discussed so far explicitly models early echoes and reverberation.
Besides, alternative models are common in the audio processing literature.

\subsection{Steering vector model}\label{subsec:processing:steering}
In the absence of echoes and reverberation, namely assuming free-field propagation,
the \acp{RIR} simplify to \textit{steering vectors}, namely the \ac{DFT} of~\cref{eq:acoustics:greenFreeTime}:
\begin{equation}\label{eq:processing:steering}
    \bsD_{j}[k] = \klist{\frac{1}{4 \pi \distMicSrc_{1j}} \cste^{-\csti 2 \pi f_k \distMicSrc_{1j} / c},
                            \cdots,
                            \frac{1}{4 \pi \distMicSrc_{Ij}} \cste^{-\csti 2 \pi f_k \distMicSrc_{Ij} / c},
                    }
\end{equation}
Furthermore, assuming far-field regimes, the microphone-to-source distance $\distMicSrc_{\idxMicSrc}$ is larger than the
inter-microphone distance $d_{ii'}$ making the attenuation factors $\sfrac{1}{4 \pi \distMicSrc_{\idxMicSrc}}$ approximately equal, hence ignored.

\subsection{Relative transfer function and interchannel models}\label{subsec:processing:rtf}
Let us consider now only two channels and only one source signal in the model~\cref{eq:processing:model:stfs}.
Dropping the dependency on $j$ for readability and taking the first channel as reference, the \ReTFdef/\sidenote{
    In the literature of spatial filtering, the Relative Transfer Function is typically denoted as RTF, which collides with the notation proposed in the thesis.
    We reminds here that we use the following notation: \acf{RTF} and \acf{ReTF}.
} associated to the $i$-th channel is defined as
the element-wise ratio of the (D)FTs of the two filters~\citeonly{gannot2001signal}
\begin{equation}\label{eq:processing:rtf}
    {G}_i[k] = \frac{{\FLT}_i[k]}{{\FLT}_1[k]}
    .
\end{equation}
The continuous-time domain counterpart is called as \ReIRdef/ and can be interpreted as the filter ``transforming'' the impulse response of the reference channel into the $i$-th one.
Considering the observation $\tilde{x}_i$ and $\tilde{x}_1$ of a single noisy and reverberant source, their signals can be re-written in term of $\tilde{g}_i$ as follows
\begin{equation}
    \begin{matrix}
    \begin{cases}
        \tilde{\mic}_1 = \tilde{\rir}_1 \convCont \tilde{s} + \tilde{\allNoise}_1 \\
        \tilde{\mic}_i = \tilde{\rir}_i \convCont \tilde{s} + \tilde{\allNoise}_i
    \end{cases} & \kto  & \begin{cases}
        \tilde{\mic}_1 = \tilde{\rir}_1 \convCont \tilde{s} + \tilde{\allNoise}_1 \\
        \tilde{\mic}_i = \tilde{\rtf}_i \convCont \tilde{\rir}_1 \convCont \tilde{s} + \tilde{\allNoise}_i
    \end{cases}
    \end{matrix}
    .
\end{equation}
Notice that $\tilde{\rir}_i = \tilde{\rtf}_i \convCont \tilde{\rir}_1$ corresponds to~\cref{eq:processing:rtf} in the frequency domain.
Moreover although the real-world \acp{RIR} $h_1$ and $h_i$ are causal, their \ac{ReTF} needs not be so.

\mynewline
The \ReTFs/ benefits of several interesting properties that will be of fundamental importance for this thesis.
In particular:
\begin{itemize}
    \item the \ac{ReTF} associated to the reference channel ($i = 1$) is equal to $1$ for each frequency bin $k$;
    \item the problem of estimating the \ac{ReTF} can be considered ``easier'' than \acp{RIR} estimation;
    In fact, in the noiseless case, it holds that $\tilde{\mic}_i = \tilde{\rtf}_i \convCont \tilde{\mic}_1$.
    Since $\tilde{\mic}_i$ and $\tilde{\mic}_1$ are both available, the \ac{ReTF} estimation boils down to transfer function estimation with known input signal;
    \item in the literature, many efficient and robust methods have been proposed to estimate \ReTFs/~\citeonly{gannot2001signal,doclo2002gsvd,markovich2009multichannel,koldovsky2015sparse,kodrasi2017evd,tammen2018iterative};
    \item a \ac{RIR} can be seen as a special case of \ac{ReTF} where the non-reference microphone is a virtual one whose
    output is the original (non-spatial) source signal $\src$. In fact, if $\rir_1 = \delta$ then $\rtf_i = \rir_i$;\sidenote{%
    In practice this virtual microphone is sometimes substituted by a microphone that is very close to the source.}
    \item as discussed below, \ReTFs/ simplify to special steering vectors in free- and far-field conditions, which have interesting geometrical properties;
    \item the \ac{ReTF} encodes acoustics properties of the related \acp{RIR} and is independent of the source signal.
    They can be used as \textit{features} for the estimation of parameters of the acoustic propagation.
    Examples of this are many works in the Sound Source Localization domain (\eg/, ~\citeonly{laufer2013relative,deleforge2015acoustic,di2019mirage}.
\end{itemize}

\mynewline
Under the narrowband approximation, the \ac{ReTF} can be approximated by the ratio of the \acp{DFT} of the channel signal $X_i[k]$ and $X_1[k]$.
\begin{equation}\label{eq:processing:inst_retf}
    G_i[k] \approx \frac{X_i[k]}{X_1[k]} \approx \frac{H_i[k]S[k]}{H_1[k]S[k]} = \frac{H_i[k]}{H_1[k]}
    .
\end{equation}
It can be noticed that ideally, the \ac{ReTF} removes the dependency from the source signal.
However, this is true only in theory.
In practice, the presence of external noise and the spectral sparsity of particular source signals (\eg/, speech, and music) make the ratio diverging from the real \ac{ReTF}.
We will refer to the \acp{ReTF} computed with this vanilla approach as \textit{instantaneous} \ac{ReTF}.
A way to improve the estimation of the \ac{ReTF} is to perform the above operation in the \ac{STFT} and then consider the average over multiple time frames.
In this way, the non-stationarity and the spectral sparsity of the speech signals are alleviated.
This approach is typically performed for computing the acoustic features known as \textit{interchannel cues}, described below.
More advanced estimators for the \ac{ReTF} have been proposed in the literature and will be discussed in~\cref{subsec:estimation:bce}.
The reader can refer also to~\cref{app:retf} for common and state of the art methods for \ac{ReTF} estimation.


\mynewline
In the general case of multiple microphone arrays ($I>2$) and multiple sources, the vector of \ReTFs/
$\boldsymbol{G}_j[k] = \ktranspose{\klist{{G}_ {1j}[k], \cdots, {G}_{Ij}[k]}}$
for the $j$-th source is defined as
\begin{equation}
    {\bsG}_j[k] = \frac{1}{{H}_{1j}[k]} {\bsH}_j[k]
    .
\end{equation}

\newthought{The relative steering vectors} are obtained by combining~\cref{eq:processing:steering,eq:processing:rtf} as
\begin{equation}\label{eq:processing:relativesteering}
    {\bsD}_{j}[k] = \klist{
                         1,
                         \cste^{-\csti 2 \pi f_k (\distMicSrc_{2j}-\distMicSrc_{i'j}) / \speedOfSound},
                         \ldots,
                         \cste^{-\csti 2 \pi f_k (\distMicSrc_{Ij}-\distMicSrc_{i'j}) / \speedOfSound}
                    }
\end{equation}
where $(\distMicSrc_{\idxMicSrc}-\distMicSrc_{1j}) / \speedOfSound$ is the \TDOA/ between the $i$-th and the reference microphones.
The \TDOAs/ will be the protagonists of~\cref{ch:mirage} as they are fundamental quantities for sound source localization.

% \newthought{\ReTFs/ estimate}
% Since~\citeonly{gannot2001signal}, many methods have been proposed to estimated it.
% A na\:ive way to estimated the \ac{ReTF} is perform the ratio between the \DFT/ of microphone pairs.
% Despite its simplicity, this methods yield estimated


\newthought{In the context of spatial auditory perception}~\citeonly{bregman1990auditory} and \ac{CASA}~\citeonly{brown2005separation}, the \ac{ReTF} is related to the \textit{interchannel cues}\sidenote{%
sometimes refers to as \textit{interaural cues} when a stress is put on the fact that the two ears are considered as receivers}.
In fact, the \ReTFs/ encodes the so-called \ac{ILD} and the \ac{IPD}
\begin{equation}
    \begin{aligned}
        \ild_{\idxMicSrc}[k] &= 20 \log_{10} \magnitudeOf{G[k]} & [\si{\dB}]\\
        \ipd_{\idxMicSrc}[k] &= \phaseOf{G[k]} \mathspace       & [\si{\radian}]
    \end{aligned}
\end{equation}
As shown in~\cref{fig:processing:ildipd}, the \ac{ILD} and the \ac{IPD} cluster around the direct path, associated to the direct path component.
However early echoes and reverberation make them significantly diverge.

\begin{figure}[]
    \begin{fullwidth}
        \includegraphics[width=\linewidth+\marginparsep]{processing/py-rtf.pdf}
        \caption{
            (Top) Synthetic \ac{RIR} for different absorption conditions for the reference channel 1.
            (Below) \ac{ILD} and \ac{IPD} related the too channel 2 computed with respect to channel 1.
            Orange lines denote the theoretical far- and free-field \ac{ILD} and \ac{IPD} as defined by the relative steering vectors of~\cref{eq:processing:relativesteering}.
            As the source is placed far from the microphones, the attenuation is similar on both the channels, whereas the delay is not.
        }\label{fig:processing:ildipd}
    \end{fullwidth}
\end{figure}


% \subsection{Probabilistic and Full-rank covariance model}
% For certain applications, it is more common to assume a statistical point of view, known as the \textit{Local Gaussian Model}\citeonly{}.
% Assume that the source STFT coefficients sj(n, f) have a zero-mean nonstationary Gaussian distribution with variance σ2
% sj (n, f), and they are all independent source-,
% frame- and frequency-wise (i.e., over j, n and f)

\section{Conclusion}
In this chapter, we presented the mathematical notions, notation, and tools to model audio scene analysis problems in signal processing.
We particularly focused on the crucial approximations and assumptions that arise from the merging of physical and signal processing models.
As we conclude the part of the thesis dedicated to background knowledge, we will now focus on estimating acoustic echoes in~\cref{pt:estimation}, and how to use them for solving audio scene analysis problems in~\cref{pt:application}.\qed