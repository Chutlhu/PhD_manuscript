\chapter{Elements of Audio Signal Processing}\label{ch:processing}
\openepigraph{Signal, a function that conveys information about a phenomenon.
$[\dots]$ Consider an acoustic wave, which can convey acoustic or music information.}{R. Priemer, \textit{Introductory Signal Processing}}
\vspace{-2.5em}
\newthought{Synopsis} Let us now move from the physics to digital signal processing.
At first this chapter formalized fundamental concepts of audio signal processing such as signal, mixtures and noise~\cref{sec:processing:model} in the time domain.
In~\cref{sec:processing:domains} we will presents the signal representation that we will use throughout the entire thesis: the STFT domain.
Finally, after assuming the narrowband approximation, in~\cref{sec:processing:rirmodels} some important models for the \RIR/ are described.
\\The notation and the material presented in this chapter are inspired and extracted from~\citeauthor{vincent2018audio}'s book \textit{Audio source separation and speech enhancement}.

\section{Signal model in the time domain}\label{sec:processing:model}
In the previous chapter we formalized the physics that rule the sound propagation from the source 'till the microphone.
A raw \textit{audio signal} encodes the variation of pressure over time on the microphone membrane.
Mathematically it is denoted as the function
\begin{equation}
    \contTime[\idxTime]{\mic} \in \R
    ,
\end{equation}
continuous both in time $t \in \R$ and amplitudes.
\marginpar{
    \centering
    \footnotesize
    \includegraphics[width=\linewidth]{processing/py_processing.pdf}
    \captionof{figure}{Continuous-time signal and its sampled version.}
    \label{fig:processing:sampling}
}

Today signals are typically processed, stored and analyzed by computers as \textit{digital audio signal}.
This corresponds to finite and discrete-time signal $\hat{x}_\idxSample$ obtained by periodically sampling the continuous-time signal $\tilde{x}$ at rate $\Fs\;[\si{\Hz}]$ and truncate it to $\idxSample$ samples.
As common to most measurement models, we assume that the sampling process involves two steps:
first, the impinging signal undergoes an ideal low-pass filter $\lowpassfilter$\sidenote{The ideal low-pass filter is\\$\lowpassfilter(t) = \sinc(t) = \sin(\pi t) / (\pi t)$} with frequency support in $\kintervoc{\sfrac{-\Fs}{2}}{\sfrac{\Fs}{2}}$;
then it time-support is regularly discretized, $t = n/\Fs$ for $n \in \Z$. This is expressed by
\begin{equation}
    \discTime[\idxSample]{\mic} = \kparen{\lowpassfilter \convCont \tilde{x}}\kparen{\frac{n}{\Fs}} \in \R
    ,
\end{equation}
where $\convCont$ is the continuous-time convolution operator.
This will restrict the frequency support of signal to satisfy the Nyquist–Shannon sampling theorem and avoid aliasing effect.
In this thesis we assume discrete signals are real-valued, ignoring the \textit{quantization} process.
\\Finally, at the end of the discretisation process, the $\tilde{x}(t)$ is represented as the finite time series or a vector,
\begin{equation}
    \hat{\mic}_{\numSample} \in \R^{\numSample}
    ,
\end{equation}
with entries $\hat{\mic}_{\numSample}[\idxSample]$ for $n = 0, \ldots, \numSample-1$.
\\The choice of $\Fs$ depends on the application since it is a trade-off between computational power, processing and rendering quality.
Historically the two iconic values are $\SI{44.1}{\kHz}$ for music distribution on CDs and $\SI{8}{\kHz}$ for first-generation speech communication.
Now multiple of $\SI{8}{\kHz}$ are typical used in audio processing: ($16, 48, 96, \SI{128}{\kHz}$).

\mynewline
Audio signals are emitted by sources and are observed, received or recorded by microphones.
A set of microphones is called a microphone \textit{array}, whose signals are sometime referred to as \textit{channels}.
In this thesis, these objects are assumed to have been deployed in a indoor environment, called generically \textit{room}.
% \begin{center}
%     \textit{All of these make our indoor \emph{auditory scene}.}
% \end{center}
Let us provide some taxonomy, through some dichotomies, useful for describe the mixing process later:

\dichotomy{Sources \vs/ Mixtures:}
Sound sources emits sounds.
When multiple sources are active at the same time, the sound that reaches our ears or is recorded using a microphone is superimposed or \textit{mixed} to a single sound.
This resulting signal is denoted as \textit{mixture}.

\dichotomy{Single-Channel \vs/ Multichannel:}
The term \textit{channel}\sidenote{%
    Please note, the term \textit{channel} has also different meaning:
    it indicates the medium in communication (\eg/ Channel Estimation)
    and sometimes one of the dimension of the input in machine learning (\eg/ image's channel).
} is used here to indicate the output of one microphones or one source.
A \textit{single-channel} signal ($\numMics = 1$) is represented by the scalar $\contTime[t]{\mic} \in \R$,
while a \textit{multichannel} ($\numMics >   1$) is represented by the vector $\contTime[t]{\mics} = \ktranspose{\klist{\contTime[t]{\mic_1}, \dots, \contTime[t]{\mic_{\numMics}}}} \in \R^{\numMics}$.

\dichotomy{Point \vs/ Diffuse Sources:}
\textit{Point sources} are emitted by a single and well-defined point in the space and their signal is single-channel.
Point sources are for instance human speakers or the sound emitted by a loudspeaker.
\\\textit{Diffuse sources} refers for instance to wind, traffic noise, or large musical instruments, which emit sound in a large region of space.
Their sound cannot be associate to a punctual source, but rather a distributed collection of them.

\dichotomy{Directional \vs/ Onmidirectional:}
An \textit{omnidirectional} source (\resp/ receiver) will in principle emit (\resp/ pick up) sound equally from all directions,
both in time and in frequency.
Although this simplify greatly processing models and frameworks, this is not true in real scenario.
The physical properties of real sources (\resp/ receivers) leads to \textit{directivity patterns}, \aka/ \textit{polarity}, which may
be different at different frequencies.
In this thesis we will assume always omnidirectional sources and receivers.

\subsection{The mixing process}
Let us assume the observed signal has $\numMics$ \textit{channels} indexed by $\idxMic \in \kbrace{1,\dots,\numMics}$.
Let us assume that there are $\numSrcs$ sources indexed by $\idxSrc \in \kbrace{1,\dots,\numSrcs}$.
Each microphone $\idxMic$ and each source $\idxSrc$ have a well defined position in the space, $\positionMicrophone_\idxMic$, $\positionSource_\idxSrc$, respectively.

The mixing process describes then the nature of the mixtures.
In order to better formalized it, \citeauthor{sturmel2012linear} introduced the intermediate representation called \emph{source spatial images}:
\begin{quote}
    \textit{$\contTime[t]{\img_{\idxMic\idxSrc}}$ describes the contribution of the source $\idxSrc$ to the microphone $\idxMic$}.
\end{quote}
Consequently, the \textit{mixture} $\contTime[t]{\mic_{\idxSrc}}$ is the possibly non-linear combination of images associated to the source $j$.
Depending on the ``contribution'' the image describes, the following type of mixture can be defined:

\dichotomy{Natural \vs/ Artificial Mixtures:}
The former refers to microphone mixtures recorded simultaneously the same auditory scene, \eg/ teleconferencing systems or hands-free devices.
By contrast, the latters are created by mixing together different individual, possibly processed, recordings.
This are the typical mixtures used professional music production where the usage of long-chain of audio effects typically ``hide'', willingly or not, the recording environment of the sound sources.

\dichotomy{Instantaneous \vs/ Convolutive Mixtures:}
In the first case, the mixing process boils down to a simple linear combination of the source signals, namely
the mixing filters are just scalar factors.
This is the typical scenario when sources are mixed using a mixing console.
\marginpar{%
    \footnotesize
    \centering
    \begin{tabular}{p{0.33\linewidth}|p{0.66\linewidth}}
    \toprule
    instantaneous   & $\tilde{\img}_ij =a_{ij} \tilde{\src}(t) $ \\
    anechoic        & $\tilde{\img}_ij =a_{ij} \tilde{\src}_j (t - \tau_{ij})$ \\
    convolutive     & $\tilde{\img}_ij =(\tilde{g}_{ij} \convCont \tilde{\src}_j) (t)$ \\
    \bottomrule
    \end{tabular}
    \captionof{table}{Taxonomy of linear mixing models for a mixture channel $x_i$,
    sources $\src_j$, impulse response $\tilde{g}_{ij}$, scaling factor $a_{ij}$ and delay $\tau_{ij}$.}
}
Convolutive mixtures, instead, denote the more general case where the each mixture is the sum of filtered signals.
In between are the \textit{anechoic} mixtures involving the sum of scaled and delayed source signals.
Natural mixtures are convolutive by nature and ideal free-far-field natural recording are well approximated by anechoic mixtures.

\begin{figure}[t]
    \begin{fullwidthfig}
        \includegraphics[width=\linewidth+\marginparsep]{processing/mixing_process.png}
    \end{fullwidthfig}

    \vspace{-\baselineskip}\vspace{-\baselineskip}
    \sideparmargin{outer}
    \sidepar{\vspace{\baselineskip}
    \caption{General mixing process, illustrated in the case of $\numSrcs = 3$ sources,
      including three point sources and one diffuse source, and $\numMics = 2$ channels.}
    }
    \label{fig:processing:mixing}
\end{figure}

\newthought{In this thesis}, we will particularly focus on natural mixture:
the microphone mixture listens to the propagation of sound in the room and this process is linear (\cfr{\cref{ch:acoustics:sec:wave}}) and time invariant provided a static scenario.
% In this case, the spatialization operation $\spat_\idxSrc(\cdot)$ is expressed by
% collection of convolution with \RIR/ $h_{\idxMic\idxSrc}$
% from source $\idxSrc$ to microphone $\idxMic$ and the post-mixing operation $\master(\cdot)$ reduces to the identity:
Therefore, the resulting mixture is the simple summation of the sound images,
which are the collections of convolution between the \RIRs/ and source signal:
\marginpar{
    \centering
        \tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt
        \resizebox{\linewidth}{!}{
            \input{figures/processing/mixing_model.tikz}
        }
        \captionof{figure}{Graphical representation of the mixing model~\ref{eq:processing:mixing:mix} for 2 sources and 2 microphones.}\label{fig:processing:mixing}
}
\begin{align}
    \tilde{\img}_{\idxMic\idxSrc}(t) &= \kparen{\tilde{\rir}_{\idxMic\idxSrc} \convCont \tilde{\src}_\idxSrc} (t)     \label{eq:processing:mixing:img} \\
    \tilde{\imgs}_\idxSrc(t)         &= \ktranspose{\klist{\tilde{\img}_{1\idxSrc}(t), \dots, \tilde{\img}_{\numMics\idxSrc}(t)}} \nonumber\\
    \tilde{\mics}(t)                 &= \sum_{\idxSrc=1}^{\numSrcs} \tilde{\imgs}_\idxSrc(t)                    \label{eq:processing:mixing:mix}
    .
\end{align}%
Considering the time domain description of the \RIR/ derived (and approximated) in the previous chapter,
the time-domain \emph{mixing filters} $\tilde{h}_{ij}( t)$ will be modeled as follows:
\begin{equation}\label{eq:processing:mixing_filter}
    \tilde{h}_{ij}( t) = \sum_{\idxEch=0}^{\numEchs} \frac{\absCoeff_{ij}^r}{4 \pi \speedOfSound \tau_{ij}^r}
                          \diracOf{t - \tau_{ij}^r} + \varepsilon_{ij}(t)
\end{equation}
where $\absCoeff_{ij}^r \in \R$ and $\tau_{ij}^r \in \R$ are the attenuation coefficient and the time delay of the reflection $\idxEch$.
The noise term $\tilde{\varepsilon}_{ij}( t)$ collects later echoes ($\idxEch > \numEchs$) and the tail of the reverberation.
We do not assume $\tilde{\varepsilon}_{ij}( t)$ to be known.

\subsection{Noise, interferer and errors}
% \openepigraph{%
%     \emph{Noise} is a general term for unwanted (and, in general, unknown) modifications that a signal may suffer during capture, storage, transmission, processing, or conversion
% }{V. Tuzlukov, \textit{Signal processing noise}}
In~\cref{eq:processing:mixing:mix} no noise is included:
all the sources are threated in the same way, including \textit{target}, \textit{interfering} and \textit{noise} sources.
While the definition of target sound source is quite self-explanatory and it will denoted by default as the first source, that is $j = 1$,
the term interfer and noise depends on the specific use case, problem, application, and research field.
Notice that in~\cref{eq:processing:mixing_filter} a noise term is added to gather unknown quantities.
\begin{quote}
    \textit{Noise is a general term for unwanted (and, in general, unknown) modifications that a signal may suffer during capture, storage, transmission, processing, or conversion}
    \citeonly{tuzlukov2018signal}.
\end{quote}
Therefore, we will define and use the following type of noises:
\marginpar{
    \centering
        \resizebox{\linewidth}{!}{
            \input{figures/processing/noise_model.tikz}
        }
        \captionof{figure}{Graphical representation of the mixing model~\eqref{eq:processing:mixing:mix}:
        $s_2(t)$ is the \textit{interferer},
        $\boldsymbol{n}( t)$ contributes to the \textit{diffuse noise field}, and
        $\boldsymbol{\varepsilon }( t)$ model acquisition and modeling errors.
        }\label{fig:processing:mixing}
}
\newthought{Interfers} identifies the undesired source with properties similar to the target source.
For instance, a concurrent speech source for speech application or concurrent music instrument in case of music.
\\Later, in this thesis the interfer sources will be denoted as additional source indexed by $j > 1$.
\newthought{Noise} collects all the remaining effects, typically nonspeech sources. Moreover we will make a further distinction between the followings.

\newthought{Diffuse Noise Field} describes the background diffuse sources present in the auditory scene, \eg/ car noise, indistinct talking or winds.
It can be recorded or approximated as \AWGN/ with a specific spatial description as described in~\citeonly{habets2007generating}.
% \\In this thesis, it is denoted as $\bsn(t) \sim \calN(0, \cov_{nn})$ where $\cov_{nn} \in \bbR^{I \times I}$ is the noise \textit{spatial covariance matrix}

\newthought{Measurement and Model Noise} accounts for general residual miss- and under-modeling error.
As common is signal processing and information theory, this error term will be modeled as \AWGN/.
\\In this thesis, it will denoted as $\tilde{\varepsilon}_{ij}( t)$ and will be used to model the
approximation of the \RIR/ with the \ISM/ or sensor noise, respectively.

\mynewline
By making the noisy terms explicit, the mixing model in~\cref{eq:processing:mixing:img,eq:processing:mixing:mix} writes:
\begin{align}
    \tilde{\img}_{\idxMic\idxSrc}(t) &=  \kparen{\tilde{\rir}_{\idxMic\idxSrc} \convCont \tilde{\src}_\idxSrc} (t) +  \tilde{\varepsilon}_{ij}(t)\\
    \tilde{\imgs}_\idxSrc(t)         &= \ktranspose{\klist{\tilde{\img}_{1\idxSrc}(t), \dots, \tilde{\img}_{\numMics\idxSrc}(t)}} \nonumber\\
    \tilde{\mics}(t)                 &= \sum_{\idxSrc=1}^{\numSrcs} \tilde{\imgs}_\idxSrc(t) + \tilde{\bsn}( t)
\end{align}

\section{Signal model in the spectral domain}\label{sec:processing:domains}\marginpar{%
    \footnotesize%
    It was introduced by Joseph Fourier in his work on the heat equation~\citeonly{fourier1822theorie}.
    His mathematical tool, named later \textit{Fourier Decomposition},
    aims at approximating any signal by a sum of sine and cosine waves.}%
The frequency, or spectral, representation is probably the most famous signal representation used in signal processing:
Speech and music signals naturally exhibit harmonic and periodic behaviors and
through it are described as combination of sinusoids as function of their frequencies.
% https://tex.stackexchange.com/questions/127375/replicate-the-fourier-transform-time-frequency-domains-correspondence-illustrati

This operation is achieved by the \FTdef/, $\fourierTrans{}:\bbR\kmapsto\bbC$, which projects a continuous-time-domain signal $\tilde{x}$ onto a space spanned by continuous-frequency complex exponentials:
\begin{equation}\label{eq:processing:ft}
    \tilde{X}(f) = (\fourierTrans{\tilde{x}})(f) =
        \int_{-\infty}^{+\infty}
        \tilde{x}(t)
        \cste^{-\csti 2 \pi f t}
        \,\kdiff{t}
    ,
\end{equation}
where $f \in \bbR$ are the \textit{natural frequency} in $\si{\Hz}$ and $\csti$ is the imaginary unit.

\marginpar{

    \centering
    \resizebox{\linewidth}{!}{
        \input{figures/processing/time2freq.tikz}
    }

    \captionof{figure}{
        A signals resolved into its Fourier series:
        a linear combination of sines and cosines
        represented as peaks in the frequency domain.
    }\label{fig:processing:mixing}
}

A part from providing a space where audio signal reveals their harmonic structures, the Fourier transforms benefits of two fundamental properties:
it is linear and it converts time-convolution into element products.
\\First, linearity allows to write~\cref{eq:processing:mixing:mix} simply as:
\begin{equation}\label{eq:processing:ft:mix}
    \tilde{\mics}( t) = \sum_{\idxSrc=1}^{\numSrcs} \tilde{\imgs}_\idxSrc( t)
    \mathspace\overset{\fourierTrans{}}{\kto}\mathspace
    \tilde{\MICS}( f) = \sum_{\idxSrc=1}^{\numSrcs} \tilde{\IMGS}_\idxSrc( f)
\end{equation}
Secondly, by the \textit{convolution theorem}, the source spatial images in~\cref{eq:processing:mixing:img} writes as:
\begin{equation}\label{eq:processing:conv}
    \tilde{\img}_{\idxMic\idxSrc}( t) =  \kparen{\tilde{\rir}_{\idxMic\idxSrc} \convCont \tilde{\src}_\idxSrc} ( t)
    \mathspace\overset{\fourierTrans{}}{\kto}\mathspace
    \tilde{\IMG}_{\idxMic\idxSrc}( f) =  \tilde{H}_{\idxMic\idxSrc}( f) \tilde{S}_\idxSrc( f)
    .
\end{equation}
As discussed in~\cref{chap:acoustics}, the \FT/ of a \RIR/, \aka/ the room transfer function, can be computed exactly in closed-form as
\begin{equation}\label{eq:processing:rir:ft}
    \tilde{H}_{ij}( f) = \sum_{\idxEch=0}^{\numEchs} \absCoeff_{ij}^r \cste^{- \csti 2 \pi f \tau_{ij}^r}
    .
\end{equation}
In practice, the filters $\tilde{h}_{ij}$ are not available in the continuous time domain nor in the continuous frequency domain directly.
They must be estimated from the observation of the discrete-time mixtures $\hat{x}_i[n]$, therefore, after the convolution with a source and the measurement process.
In practice, we don't have access to continuous signal, neither is time and in frequency domain.
Every signal or spectrum the microphones capture are represented by finite- and discrete time signals for which the properties~\eqref{eq:processing:conv} are valid with some precautions.
% That gives rise to the field of digital signal processing.

\subsection{Discrete time and frequency domains}
% Since microphones and acquisition systems operate with finite and discrete number of samples, continuous signals cannot be observed, only approximated.
The spectral representation of a discrete- and finite-time signal $\hat{x}_N$ is given by its (forward) \DFTdef/
\sidenote{
    This can be intrepreded as the projection onto the space spanned by a finite number of complex exponentials.
},
$\discreteFT{}:\bbR^N\kmapsto\bbC$:
\begin{equation}\label{eq:processing:dft}
    \hat{X}_F[ k] = (\discreteFT{\hat{x}_N})[k] =
    \sum_{n = 0}^{N - 1}
    \hat{x}_N[ n]
    \cste^{-\csti2\pi k n / F}.
\end{equation}
where $k \in \kintervcc{0}{F - 1}$ is the discrete \textit{frequency bin} and $F$ is the total number of bins.
Again we use the subscript $F$ and the brackets $[k]$ to stress the finite and discrete frequency support of the $\DFT/$.
\\The natural frequency $f_k$ in $\si{\Hz}$ corresponding to the $k$-th frequency bin can be computed as
\begin{equation}\label{eq:processing:fk}
    f_k = \frac{k}{F}\Fs
    .
\end{equation}

\subsection{The DFT as approximation of the FT}\label{subsec:processing:ftapprox}
An important application of the \DFT/ is to approximate numerically the \FT/.
As mentioned at the beginning of the chapter, with the discretisation process the continuous signal is periodically sampled, low-passed and finally truncated.
It can be proved that sampling in the time domain corresponds to limiting the signal bandwidth and periodizing the spectrum.
\\By assuming sampling at rate $\Fs$, in the continuos-frequency domain the spectrum $\tilde{X}(f)$ is repeated every internals of size $\Fs\,\si{\Hz}$ .
By further assuming that the signal undergoes an ideal low-pass filter, no spectral leakage is presents between each repetition.
\\So far, the sampled time domain signal, $\hat{x}[n]$, is mapped to the continuous frequency domain $\tilde{X}(f)$.
This particular case of the \FT/ is called \DTFTdef/ and it is denote with $\tilde{X}_{\Fs}[ k]$.
\begin{equation}\label{eq:processing:ft2dtft}
    \begin{aligned}
        \tilde{X}(f) =
        \int_{-\infty}^{+\infty}
        \tilde{x}(t)
        \cste^{-\csti 2 \pi f t}
        \,\kdiff{t}
        \quad \rightarrow \quad
        \tilde{X}_{\Fs}( f) =
        \sum_{n = -\infty}^{\infty}
        \hat{x}[ n]
        \cste^{-\csti2\pi f \frac{n}{\Fs}}
    \end{aligned}
    .
\end{equation}
Here the continuous integral the \FT/ is approximated by Riemann sum over the discrete points $n \in \Z$:
To be more rigorous, when computing a Riemann sum approximation, the length of the discretisation interval multiply the summation.
In our application, this quantity always set to $\Fs$ and for readability reason such term is dropped.
\\The quality of this approximation \wrt/ the original continuous spectrum is regulated by the choice of $\Fs$: the higher $\Fs$, the better the approximation.
The upper bound to the possible value $\Fs$ is the results known as the Nyquist–Shannon's sampling theorem.

\mynewline
Furthermore, we consider only the finite sequence $\hat{x}_N$ consisting of $N$ samples.
This would reduce the summation ranges the right part of~\cref{eq:processing:ft2dtft}.
Instead, we can keep the infinite summation by multiplying the sampled signal by a discrete-time window function $\hat{w}$ selecting the non-zero porting of $\hat{x}$, $\hat{x}_N[n] = \hat{w}[n]\hat{x}[n]$.
By the \textit{convolution theorem}, the multiplication in the time domain translates in a convolution between the corresponding spectra.
As a consequence, the spectrum of the truncated signal is distorted by the spectrum of the window function.
In math,
\begin{equation}\label{eq:processing:ft2dtft}
    \tilde{X}_N(f) =
    \sum_{n=0}^{N-1}
    \hat{x}_N[n]
    \cste^{-\csti2\pi f \frac{n}{\Fs}}
    \quad \leftrightarrow \quad
    \tilde{X}_{\Fs}( f) =
    \sum_{n = -\infty}^{\infty}
    \hat{x}[ n] \hat{w}[ n]
    \cste^{-\csti2\pi f \frac{n}{\Fs}}
    .
\end{equation}
By the convolution theorem, we have that
\begin{equation}
    \hat{x}_N[n] = \hat{x}[ n] \hat{w}[ n]
    \quad \leftrightarrow \quad
    \tilde{X}_{N}(f) = (\tilde{X}_{\Fs} \convCont \tilde{W}_{\Fs})(f)
\end{equation}
where $\tilde{W}_{\Fs}$ is the \DTFT/ of the sampled window function $\hat{w}[n]$.
\\Assuming the window function to be an ideal door function\sidenote{door function here}, its \DTFT/ is a ideal low-pass filter, which acts on the original spectrum as a smoothing function.
As a consequence, the quality of this approximation is then based on the spectral leakage of the chosen window function, $w[n]$.
As a rule of thumb, here the longer the segment, the better the approximation%
\sidenote{When short excerpt are considered instead (\eg/ in case of the \STFTdef/), particular types of window function are used but their analysis are out of the scope of this thesis.}

\mynewline
Finally, we cannot access the \DTFT/ directly because that involves an infinite number of frequencies $f \in \R$.
Therefore, taking $F$ uniformly-spaced frequency $f_k\in\R$ as in~\cref{eq:processing:fk}, we finally obtain the \DFT/ as in~\cref{eq:processing:dft}, that is
\begin{equation}
    \tilde{X}_N(f_k) =
    \sum_{n=0}^{N-1}
    \hat{x}_N[n]
    \cste^{-\csti2\pi f \frac{n}{\Fs}}
    \quad \leftrightarrow \quad
    \hat{X}_F[k] =
    \sum_{n=0}^{N-1}
    \hat{x}_N[n]
    \cste^{-\csti2\pi k n / F}
    .
\end{equation}
Notice that the $\Fs$ term disappeared in the right part of the equation above as it cancels out when using~\cref{eq:processing:fk}.
By increasing $F$, we can sample more densely $\hat{X}_F[k]$ which leads to a better approximation to $\tilde{X}_N$.
However this does not eliminates the distortion of the previous steps, due to $\tilde{W}_{\Fs}$.

\mynewline
Again, we sampled a domain. Thus, according to the defined sampling process, this involve using a ideal low-pass filter.
This filter acts now on the discrete spectrum, smoothing it and limiting the support of its transformation in the dual domain.
Therefore, the inverse \DFT/ of $\hat{X}_F[k]$ is not properly $\hat{x}_N[n]$, but its periodic version repeated every $F$ samples.
In fact, sampling in one of the two domain is equivalent to a periodization in the other domain while truncating lead to convolving with a window function.
Moreover, the chain of operation (sampling in time and truncation in time and sampling in frequencies) are valid in both way.
Thus one can arbitrarily first sample and truncate frequency domain and finally sample in time.
The only difference is in the interpretation of the windowing function, which in one case smooth the spectrum and in the other smooth the signal.
All this relation and approximation that connects the \FT/ to the \DFT/ are well explained in explanatory material presented in~\sidenote{\url{https://krasjet.com/rnd.wlk/poisson.pdf}}.

% Through the use of microphones and computers, our window into the continuous physical world of acoustics is narrow and discrete, both it time and in frequency.

\subsection{Signal model in the discrete Fourier domain}
Conscious of the above approximations, we can now rewrite our signal model for the discrete case.
Hereafter we will always consider finite-length sequences and the index $N$ will be dropped to lighten the notation.
\\The \DFT/ is linear, so the discrete version of \cref{eq:processing:ft:mix} becomes
\begin{equation}\label{eq:processing:dft:mix}
    \hat{\mics}[ n] = \sum_{\idxSrc=1}^{\numSrcs} \hat{\imgs}_\idxSrc[ n]
    \mathspace\overset{\discreteFT}{\kto}\mathspace
    \hat{\MICS}[ k] = \sum_{\idxSrc=1}^{\numSrcs} \hat{\IMGS}_\idxSrc[ k]
\end{equation}
Secondly, by using na\"ively the discrete convolution theorem, one could translate \cref{eq:processing:mixing:img} as
\begin{equation}\label{eq:processing:discreteModel}
    \hat{\img}_{ij}[n] = (\hat{\rir}_{ij} \convDis \hat{\src})[n]
    \mathspace\overset{\discreteFT{}}{\kto}\mathspace
    \hat{C}_{ij}[k] = \hat{H}_{ij}[k] \hat{S}[k]
    ,
\end{equation}
where $\convDis$ is the finite-time linear convolution operator\sidenote{
    The finite-time linear convolution for two vectors $\hatu\in\bbR^L$ and $\hatv\in\bbR^D$ is
    \\$(\hatu \convDis \hatv)[n] = \sum_{l=0}^{L-1} \hatu[l] \hatv[L-1+n-j]$ for $n = 0, \cdots, D-L$
    .
}.
\\The filter $\hat{H}_{ij}[k]$ is the \DFT/ of the room impulse response.
As mentioned in the~\cref{subsec:processing:ftapprox}, this just approximates the room transfer function of~\cref{eq:processing:rtf}.
Thus we can write,
\begin{equation}\label{eq:processing:discreteModel:rir}
    \hat{H}_{ij}[ k] \approx \sum_{\idxEch=0}^{\numEchs}
                \frac{\absCoeff_{ij}^r}{4 \pi \speedOfSound \tau_{ij}^r}
                \cste^{- \csti 2 \pi k \Fs \tau_{ij}^r / F}
    .
\end{equation}

\mynewline
Although used in practice, the model~\eqref{eq:processing:discreteModel} makes use of other approximations that are worth presenting.
In particular, the work by~\citeonly{tukuljac2018mulan} properly discuss them in the context of the echo estimation problem.
The paper mention three approximations, which are depicted in the following diagram.

\begin{figure}[!h]
    \begin{fullwidthfig}
    \centering
    \input{figures/processing/approx_diagram.tikz}
    \end{fullwidthfig}
\end{figure}

The diagram shows a chain of operators (sampling and transforms) with provable and approximated equivalences that lead to~\cref{eq:processing:discreteModel} used in practice.
In order,
\begin{enumerate}[label=(\roman*)]
    \item\label{it:processing:dft:approx1}
    In~\citeonly{van2001gaussian}, the Proposition 2 shows that if the signal $\tilde{\src}(t)$ is band-limited by $\Fs$,
    then sampling the continuous convolution is exactly equivalent to \textit{linearly convolving} the infinite discrete signal $\hat{\src}[n]$ and the discrete and low-passed version of the filter.
    While the source signal is band-limited by nature, $\tilde{h}(t)$ is not (in fact the \RIR/ is modeled as a summation of spikes, which has infinite spectrum).
    Thus, the first approximation~\ref{it:processing:dft:approx1} considers $\hat{h}[n] \approx (\lowpassfilter \convDis \tilde{h})[n]$, in words
    we assume that the filter is band-limited by $\pm \sfrac{\Fs}{2}$.
    \\\citeauthor{tukuljac2018mulan} made an important observation here:
    even if infinite number of samples are available, after the measurement process, the discrete-time filter $\hat{\rir}[n]$ consists of infinite-length decimated combinations of sinc functions.
    \\In the context of this thesis, this observation tell us that even in ideal conditions, that is without noise, possibly knowing the transmitted signal, and processing infinitely
    many samples, the exact estimation of the echo properties of the \RIR/ is challenging task itself. This is a fundamental difference between \RIR/ estimation and estimating the
    time of arrivals of the early echoes.
    \\Note, for instance, that we wrote the echo model only in the continuous-time domain or with its closed-form form discrete frequencies.
    The discrete-time domain was avoided on purpose since the echoes' arrival time are naturally off the sampling grid, namely not integer multiple $\Fs$.

    \item\label{it:processing:dft:approx2}
    The discrete-time convolution theorem applies to the \textit{circular convolution}, which can be approximated by the \textit{linear convolution}
    that is $(\hath \circledast \hats)[n] \approx (\hath \convDis \hats)[n]$.
    This second approximation is reasonably good when many samples are available and when one of the two signals is periodic, which
    are typical cases for audio signals.

    \item\label{it:processing:dft:approx3}
    The third approximation regards the closed-form of $h_{ij}(f)$ of~\cref{eq:processing:discreteModel:rir} which
    would require infinitely many samples and unlimited frequency support to be computed\sidenote{This formula would results from the \DTFTdef/ of $\tilde{\rir}_{ij}(t)$}.
\end{enumerate}

Nevertheless, it is important to notice that approximations \ref{it:processing:dft:approx2} and \ref{it:processing:dft:approx3} become arbitrarily precise as the number of samples $N$ grows to infinity.

\mynewline
While the raw audio signal encodes the amplitude of a sound as a function of time,
its spectrum represents it as a function of frequency.
In order to jointly account for both temporal and spectral characteristic, joint time-frequency representations are used.

\subsection{Time-Frequency domain representation}
\TFdef/ representations aim to jointly describe the signal in the time and frequency domains.
Instead of considering the entire signal, the main idea is to consider only a small section of the signal.
To this end, one fixes a so-called \textit{window} function, $\hatw_N[n]$, which is nonzero for only a period of time $\winsize$ shorter than the entire signal length, $\winsize \ll N$.
This function iteratively shifts and multiplies the original signal, producing consecutive \textit{frames}.
Finally, the frequency information are extracted independently from each frame.
The choice of a window function $w[n]$ depends on the application since its contribution reflects in the \TF/ representation together with the
one of the signal.

\newthought{The discrete \STFT/}\marginpar{\footnotesize%
The \STFT/ was introduced by Dennis Gabor in the 1946, the person behind Holography and Gaborlets.
}
is the most commonly used \TF/-representation in audio signal processing.
This representation encodes the time-varying spectra into a matrix $X[k,l] \in \bbC^{F,T}$ with frequency index $k$ and time frame index $l$.
More formally, the process to compute the complex \STFT/ coefficients is given by
\begin{equation}\label{eq:processing:stft}
    X[k, l]  = \sum_{n=0}^{\winsize-1} w[n] x[n + l \hopsize] \cste^{- \csti 2 \pi k n / F} \mathspace\in\bbC
\end{equation}
where $\winsize$ is the window length and $\hopsize$ is the \textit{hop size} which specifies how much the window needs to be shifted across the signal.
Equivalently, \cref{eq:processing:stft} can be expressed as \DFT/s of windowed frames, $X[k, l] = \discreteFT{\hatx[n,l]}$ where $\hatx[n,l] = \hatx[n + l \hopsize] \hatw[n]$.

Since each \STFT/ coefficient $x[k, l]$ lives in the complex space $\bbC$, the squared magnitude of the \STFT/, $\powerOf{\hatX[k,l]}$ is
commonly used for visualization and for processing
\marginpar{
    \centering
    \footnotesize
    \includegraphics[width=\linewidth]{processing/py-speech_spectrogram.pdf}
    \captionof{figure}{STFT spectrogram of an example speech signal. Higher energies are illustrated with darker colors.}
    \label{fig:processing:spectrogram}
}%\sidenote{The phase in some application is completely ignored because of difficult interpretation or commonly considered less informative with respect to the sound magnitude}.
The resulting two-dimensional representation is called (log) \textit{spectrogram}.
It can be visualized by means of a two-dimensional image, whose axes represent time frames and frequency bins.
In this image, the (log) value $\powerOf{\hatX[k,l]}$ is represented by the intensity or color in the image at the coordinate $[k,l]$.
Throughout this works both estimation and processing will be typically conducted in the \STFT/ domain, unless specified.
This is a common approach in the audio signal processing community, but it is not the only one:
many algorithm are designed directly in the time domain or in alternatives \TF/ representation, \eg/ Mel-Scale, Filter-Banks, or the quadratic STFT transform used in~\cref{ch:brioche}.

As discussed~\citeonly{vincent2018audio}, the \STFT/ has the following useful properties for audio processing:
\begin{itemize}
    \item the frequencies $f_k$ is a linear function of the frequency bin $k$;
    \item the resulting matrix allows easy treatment of the phase $\phaseOf{\hatX[k,l]}$, the magnitude $\magnitudeOf{\hatX[k,l]}$ and the power $\powerOf{\hatX[k,l]}$ separately;
    \item the \DFT/ can be efficienciently computed with the \FFT/ algorithm;
    \item the \STFT/ is simple to invert;
    \item the \STFT/ inherits the linearity and convolution property of the \DFT/ under some condition about the length of the signals.
\end{itemize}
\marginpar{%
    \vspace{-4cm}
    \footnotesize
    For more mathematical detailed description on \DFT/ and \STFT/ can be found in \citeonly{oppenheim1987signals}.
    For a audio-processing-oriented and music-processing-oriented explanation please refer to Chapter 2 of \citeonly{vincent2018audio} (Chapter2) and Chapter 2 of \citeonly{muller2015fundamentals}, respectively.
}

\begin{figure}[t]
    \begin{fullwidthfig}
    \centering
        \input{figures/processing/diagram_stft.tikz}
    \end{fullwidthfig}
\end{figure}


\subsection{The final model}
The model \eqref{eq:processing:discreteModel} shows how in practice the RIRs are treated in the frequency-domain.
However this does not generalize straightforwardly to the time-frequency domain:
it depends on the length of the filter \wrt/ to the length of the analysis window on of the \STFT/.
Issues arise with ``long'' filters, which are common in highly reverberant or time-varying scenarios.
To circumvent this issue, the \textit{convolutional STFT} for arbitrary window functions have been proposed\sidenote{%
It translates the time-domain convolution into inter-frame and inter-band convolutions, rather than pointwise multiplication of Fourier transforms.
}~\citeonly{gilloire1992adaptive}.
Although mathematically exact, it is computationally and memory intensive.

In this thesis, we will assume that the filter length is shorter than the analysis window length.
In the literature, this is known as the \textit{narrowband approximation}, namely the time-domain filtering can be approximated by complex-valued multiplication in each time-frequency bin $[l,k]$:
\begin{equation}
    \IMGS_j[l,k] \approx \hat{\FLTS}[k] \SRC_j[l,k]
    ,
\end{equation}
where the $\hat{\FLTS}_j[k] = \ktranspose{\klist{\hat{\flt}_{1j}[k], \cdots, \hat{\flt}_{Ij}[k]}}$ is the $I \times 1$ vector of the room transfer functions for source $j$.
It is sometimes practical to concatenate all these vectors into an $I \times J$ matrix $\hat{\FLTSS}[k] = \klist{\FLTS_1(f),\cdots,\FLTS_J(f)}$ called \textit{mixing matrix}.

With the above notation and considerations, mixing process including noise terms can be written in the \STFT/ domain compactly as:
\begin{equation}\label{eq:processing:model:stfs}
    \MICS[l,k] = \FLTSS[l,k] \SRCS[l,k] + \bsU[l,k]
\end{equation}
where $\bsU[l,k] = \bsN[l,k] + \boldsymbol{\varepsilon}(l,k)$ includes the contribution of both diffuse noise sources, modeling and measurement errors.

% The above equation can be seen as a \textit{deterministic} parametrization of the mixing process and it does not consider a statistical description
% of the reverberation. In fact it the filter

% \newthoughtpar{The spatial quadratic STFT transform}
% \begin{equation}
%     \cov_{\mics}[l,k] = \bbE \klist{\mics[l,k]\khermitian{\mics[l,k]}}
% \end{equation}

\section{Other (room) impulse response spectral models}\label{sec:processing:rirmodels}
\RIRs/ are complicated quantities to model, compute and estimate.
The representations of the \RIR/ discussed so far explicitly models early echoes and reverberation deterministically.
Furthermore, alternative models are common in the audio processing literature.

\subsection{Steering vector model}
In the absence of echoes and reverberation, namely assuming free-field propagation,
the \RIRs/ simplify to \textit{steering vectors}, namely the DFT of~\cref{eq:acoustics:greenFreeTime}:
\begin{equation}\label{eq:processing:steering}
    \bsD_{j}[k] = \klist{\frac{1}{4 \pi \distMicSrc_{1j}} \cste^{-\csti 2 \pi f_k \distMicSrc_{1j} / c},
                            \cdots,
                            \frac{1}{4 \pi \distMicSrc_{Ij}} \cste^{-\csti 2 \pi f_k \distMicSrc_{Ij} / c},
                    }
\end{equation}
Furthermore, assuming far-field regimes, the microphone-to-source distance $\distMicSrc_{ij}$ is larger than the
inter-microphone distance $d_{ii'}$ making the attenuation factors $\sfrac{1}{4 \pi \distMicSrc_{ij}}$ approximately equal, hence ignored.

\subsection{Relative transfer function and interchannel models}\label{subsec:processing:rtf}
Let us consider now only two channels and only one source signal in the model~\cref{eq:processing:model:stfs}.
Dropping the dependency on $j$ for readability and taking the first channel as reference, the \RTFdef/ associated to the $i$-th channel is defined as
the element-wise ratio of the (D)FTs of the two filters~\citeonly{gannot2001signal}
\begin{equation}\label{eq:processing:rtf}
    \hat{G}_i[k] = \frac{\hat{\FLT}_i[k]}{\hat{\FLT}_1[k]}
    .
\end{equation}

The continuous-time domain counterpart is called as \ReIRdef/ and can be interpreted as the filter ``transforming'' the $i$-th impulse response into the one of the reference channel.
Considering the noisy observation $\tilde{x}_i$ and $\tilde{x}_1$, their signals can be re-written in term of $\tilde{g}_i$ as follows
\begin{equation}
    \begin{matrix}
    \begin{cases}
        \tilde{\mic}_1 = \tilde{\rir}_1 \convCont \tilde{s} + \tilde{\allNoise}_1 \\
        \tilde{\mic}_i = \tilde{\rir}_i \convCont \tilde{s} + \tilde{\allNoise}_i
    \end{cases} & \kto  & \begin{cases}
        \tilde{\mic}_1 = \tilde{\rir}_1 \convCont \tilde{s} + \tilde{\allNoise}_1 \\
        \tilde{\mic}_i = \tilde{\rtf}_i \convCont \tilde{\rir}_i \convCont \tilde{s} + \tilde{\allNoise}_i
    \end{cases}
    \end{matrix}
    .
\end{equation}
Notice that $\tilde{\rir}_i = \tilde{\rtf}_i \convCont \tilde{\rir}_1$ corresponds to~\cref{eq:processing:rtf} in the frequency domain.
Moreover although the real-world \RIRs/ $h_1$ and $h_i$ are causal, their \RTF/ needs not be so.

The \RTFs/ benefits of several interesting properties that will be of fundamental importance for this thesis.
In particular:
\begin{itemize}
    \item the \RTF/ associated to the reference channel ($i = 1$) is equal to $1$ for each frequency bin $k$.
    \item The problem of estimating the \RTF/ can be considered ``easier'' than \RIRs/ estimation.
    In fact, in the noiseless case, it holds that $\tilde{\mic}_i = \tilde{\rtf}_i \convCont \tilde{\mic}_1$.
    \item The \RTFs/ encode properties of the related impulse responses and there are many efficient methods to estimate them\sidenote{%
        In~\cref{ch:brioche} methods for estimation the RTF will be discussed
    }.
    Therefore, it may be used as a proxy for the estimations of (components of) \RIRs/.
    \item A \RIR/ can be seen as a special case of \RTF/ where the non-reference microphone is a virtual one whose
    output is the original (non-spatial) source signal $\src$. In fact, if $\rir_1 = \delta$ then $\rtf_i = \rir_i$\sidenote{%
    In practice this virtual microphone is sometimes substituted by a microphone that is very close to the source.}.
    \item As discussed below, \RTFs/ simplify to special steering vectors in free- and far-field conditions, which have interesting geometrical properties.
\end{itemize}

In the general case of multiple microphone arrays ($I>2$) and multiple sources, the vector of \RTFs/
$\boldsymbol{G}_j[k] = \ktranspose{\klist{\hat{G}_ {1j}[k], \cdots, \hat{G}_{Ij}[k]}}$
for the $j$-th source is defined as
\begin{equation}
    \hat{\bsG}_j[k] = \frac{1}{\hat{G}_{1j}[k]} \hat{\bsG}_j[k]
    .
\end{equation}


\newthought{The relative steering vectors} are obtained by combining~\cref{eq:processing:steering,eq:processing:rtf} as
\begin{equation}\label{eq:processing:relativesteering}
    \hat{\bsD}_{j}[k] = \klist{
                         1,
                         \cste^{-\csti 2 \pi f_k (\distMicSrc_{2j}-\distMicSrc_{i'j}) / \speedOfSound},
                         \ldots,
                         \cste^{-\csti 2 \pi f_k (\distMicSrc_{Ij}-\distMicSrc_{i'j}) / \speedOfSound}
                    }
\end{equation}
where $(\distMicSrc_{ij}-\distMicSrc_{1j}) / \speedOfSound$ is the \TDOA/ between the $i$-th and the reference microphones.
The \TDOAs/ will be the protagonists of~\cref{chap:mirage} as they are fundamental quantities for sound source localization.

\newthought{In the context of spatial auditory perception} and \CASA/, the \RTF/ is related to the \textit{interchannel cues}\sidenote{%
sometimes refers to as \textit{interaural cues} when a stress is put on the fact that the two ears are considered as receivers}.
In fact, the \RTFs/ encodes the so-called \ILD/ and the \IPD/
\begin{equation}
    \begin{aligned}
        \ild_{ij}[k] &= 20 \log_{10} \magnitudeOf{\rtf[k]} & [\si{\dB}]\\
        \ipd_{ij}[k] &= \phaseOf{\rtf[k]} \mathspace       & [\si{\radian}]
    \end{aligned}
\end{equation}

As shown in~\cref{fig:processing:ildipd}, the \ILD/ and the \IPD/ cluster around the direct path, associated to the direct path component.
However early echoes and reverberation make them significantly diverge.

\begin{figure}[b]
    \begin{fullwidthfig}
        \includegraphics[width=\linewidth+\marginparsep]{processing/py-rtf.pdf}
        \caption{
            \RIR/, \ILD/ and \IPD/ corresponding to the pair of synthetic impulse responses of~\cref{fig:acoustics:rir} for different absorption conditions.
            Orange lines denote the theoretical far- and free- field \ILD/ and \IPD/ as defined by the relative steering vectors of~\cref{eq:processing:relativesteering}
        }\label{fig:processing:ildipd}
    \end{fullwidthfig}

    % \vspace{-\baselineskip}\vspace{-\baselineskip}
    % \sideparmargin{outer}
    % \sidepar{\vspace{\baselineskip}
\end{figure}


% \subsection{Probabilistic and Full-rank covariance model}
% For certain applications, it is more common to assume a statistical point of view, known as the \textit{Local Gaussian Model}\cite{}.
% Assume that the source STFT coefficients sj(n, f) have a zero-mean nonstationary Gaussian distribution with variance σ2
% sj (n, f), and they are all independent source-,
% frame- and frequency-wise (i.e., over j, n and f)